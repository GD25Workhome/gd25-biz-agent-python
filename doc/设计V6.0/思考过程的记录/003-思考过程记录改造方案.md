# 思考过程记录改造方案

## 1. 背景与目标

### 1.1 背景

基于"002-LangChain思考过程提取可行性深度分析.md"的结论，我们已经验证了：
- ✅ LangChain 的 `_convert_dict_to_message` 函数确实会丢弃 `reasoning_content` 字段
- ✅ Monkey Patch 方案可以有效保留 `reasoning_content` 字段
- ✅ `additional_kwargs` 机制本身正常工作，可以存储自定义字段

### 1.2 目标

实现思考过程的完整记录，包括：
1. **Logger 打印**：在控制台日志中显示思考过程
2. **Langfuse 记录**：在 Langfuse 中记录思考过程，便于可观测性分析
3. **数据库存储**：将思考过程保存到数据库（可选，通过 `response_snapshot` 或 `response_messages`）

### 1.3 技术方案

采用**方案1：Monkey Patch `_convert_dict_to_message`**（已验证可行）：
- 实现简单，代码量少
- 思考过程直接出现在 `AIMessage.additional_kwargs` 中
- 与现有代码兼容性好

## 2. 改造范围

### 2.1 需要修改的文件

1. **新建文件**：`infrastructure/llm/reasoning_patch.py`
   - 实现 Monkey Patch 逻辑
   - 提供 `apply_reasoning_patch()` 函数

2. **修改文件**：`infrastructure/llm/client.py`
   - 在文件开头应用 Monkey Patch
   - 确保在导入 `ChatOpenAI` 之前执行

3. **修改文件**：`infrastructure/observability/llm_logger.py`
   - 增强从 `additional_kwargs` 提取 `reasoning_content` 的逻辑
   - 确保思考过程被正确记录到日志和数据库

4. **修改文件**：`infrastructure/observability/langfuse_handler.py`
   - 在 Langfuse Callback 中提取并记录思考过程
   - 通过 `metadata` 或自定义字段传递给 Langfuse

### 2.2 不需要修改的文件

- 数据库模型：`response_snapshot` 和 `response_messages` 已支持存储思考过程
- 业务代码：无需修改，自动生效

## 3. 详细实现方案

### 3.1 步骤1：创建 reasoning_patch.py 模块

**文件路径**：`infrastructure/llm/reasoning_patch.py`

**功能**：
- 实现 Monkey Patch 逻辑
- 替换 LangChain 的 `_convert_dict_to_message` 函数
- 在保留原有逻辑的基础上，额外保留 `reasoning_content` 字段

**实现代码**：

```python
"""
LangChain reasoning_content 提取补丁

通过 monkey patch 的方式，让 LangChain 保留 reasoning_content 字段
"""
import logging
from typing import Mapping, Any

logger = logging.getLogger(__name__)

# 全局标志，确保只应用一次
_patch_applied = False


def apply_reasoning_patch() -> bool:
    """
    应用 reasoning_content 提取补丁
    
    必须在导入 ChatOpenAI 之前调用
    
    Returns:
        bool: 是否成功应用补丁
    """
    global _patch_applied
    
    if _patch_applied:
        logger.debug("reasoning_content 补丁已应用，跳过重复应用")
        return True
    
    try:
        from langchain_openai.chat_models.base import _convert_dict_to_message as _original_convert
        import langchain_openai.chat_models.base
        from langchain_core.messages import AIMessage
        
        def _enhanced_convert_dict_to_message(_dict: Mapping[str, Any]):
            """
            增强版的消息转换函数，保留 reasoning_content
            
            原理：在调用原始函数后，检查原始字典中是否有 reasoning_content，
            如果有，则将其添加到 additional_kwargs 中
            """
            # 调用原始函数
            message = _original_convert(_dict)
            
            # 如果是 AIMessage 且原始字典包含 reasoning_content
            if isinstance(message, AIMessage) and 'reasoning_content' in _dict:
                reasoning_content = _dict.get('reasoning_content')
                if reasoning_content:
                    # 确保 additional_kwargs 存在
                    if not hasattr(message, 'additional_kwargs') or message.additional_kwargs is None:
                        message.additional_kwargs = {}
                    # 添加 reasoning_content
                    message.additional_kwargs['reasoning_content'] = reasoning_content
                    logger.debug(f"已提取 reasoning_content，长度: {len(reasoning_content)} 字符")
            
            return message
        
        # 应用 monkey patch
        langchain_openai.chat_models.base._convert_dict_to_message = _enhanced_convert_dict_to_message
        _patch_applied = True
        logger.info("✅ 已应用 reasoning_content 提取补丁")
        return True
    except Exception as e:
        logger.error(f"❌ 应用 reasoning_content 补丁失败: {e}", exc_info=True)
        return False
```

**关键点**：
- 使用全局标志 `_patch_applied` 确保只应用一次
- 在调用原始函数后，检查并添加 `reasoning_content`
- 确保 `additional_kwargs` 存在后再添加字段
- 添加详细的日志记录

### 3.2 步骤2：在 client.py 中应用补丁

**文件路径**：`infrastructure/llm/client.py`

**修改内容**：
- 在文件开头（导入其他模块之前）导入并应用补丁
- 确保在创建 `ChatOpenAI` 实例之前执行

**修改位置**：在文件开头，`get_llm` 函数之前

**实现代码**：

```python
"""
LLM 客户端封装
支持 DeepSeek API 和其他兼容 OpenAI 的 API
"""
import logging
from typing import Optional, List, Any
from langchain_openai import ChatOpenAI
from langchain_core.language_models import BaseChatModel

from app.core.config import settings
from infrastructure.observability.llm_logger import (
    LlmLogCallbackHandler,
    LlmLogContext,
)

# 应用 reasoning_content 提取补丁（必须在导入 ChatOpenAI 之前）
from infrastructure.llm.reasoning_patch import apply_reasoning_patch

# 应用补丁（只执行一次）
_patch_applied = False
if not _patch_applied:
    apply_reasoning_patch()
    _patch_applied = True

logger = logging.getLogger(__name__)

# ... 其余代码保持不变 ...
```

**关键点**：
- 补丁必须在导入 `ChatOpenAI` 之前应用
- 使用模块级标志避免重复应用
- 如果补丁应用失败，记录错误但不影响主流程

### 3.3 步骤3：增强 llm_logger.py 中的提取逻辑

**文件路径**：`infrastructure/observability/llm_logger.py`

**修改内容**：
- 在 `on_llm_end` 方法中，优先从 `additional_kwargs` 提取 `reasoning_content`
- 确保思考过程被正确记录到控制台日志和数据库

**修改位置**：`on_llm_end` 方法中，提取响应内容的部分（约第 364-418 行）

**实现代码**：

```python
def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
    """LLM 成功结束回调"""
    # ... 现有代码 ...
    
    # 提取响应内容和思考过程
    response_text = None
    reasoning_content = None  # DeepSeek R1 的思考过程
    response_messages: List[Dict[str, Any]] = []
    try:
        generations = response.generations if hasattr(response, "generations") else []
        if generations and generations[0]:
            message = generations[0][0].message
            content = getattr(message, "content", None)
            
            # ===== 优先从 additional_kwargs 提取 reasoning_content =====
            # 这是 Monkey Patch 方案的核心：reasoning_content 会被放入 additional_kwargs
            if hasattr(message, "additional_kwargs"):
                additional_kwargs = getattr(message, "additional_kwargs", {}) or {}
                reasoning_content = additional_kwargs.get("reasoning_content")
                if reasoning_content:
                    logger.debug(f"✅ 从 additional_kwargs 中提取到 reasoning_content，长度: {len(reasoning_content)} 字符")
            
            # 处理结构化内容（当使用 reasoning 参数时，content 可能是列表）
            if isinstance(content, list):
                # ... 现有代码保持不变 ...
            elif content:
                # ... 现有代码保持不变 ...
            
            # 如果还没有找到思考过程，尝试其他方式（保留原有逻辑作为后备）
            if not reasoning_content:
                # 尝试从 <think> 标签中提取（保留原有逻辑）
                # ... 现有代码 ...
                
                # 尝试从 response_metadata 中查找（保留原有逻辑）
                # ... 现有代码 ...
                
                # 尝试从全局存储的原始响应中提取（保留原有逻辑）
                # ... 现有代码 ...
            
            # 记录响应消息
            if response_text:
                response_messages.append({
                    "role": getattr(message, "type", "assistant"),
                    "content": response_text,
                    "token_estimate": _estimate_tokens(response_text),
                })
            
            # 记录思考过程消息（如果存在）
            if reasoning_content:
                response_messages.append({
                    "role": "reasoning",
                    "content": reasoning_content,
                    "token_estimate": _estimate_tokens(reasoning_content),
                })
    except Exception as e:
        logger.debug(f"响应内容解析失败: {str(e)}", exc_info=True)
    
    # ... 其余代码保持不变 ...
    
    # 打印思考过程（如果存在）
    if reasoning_content:
        reasoning_str = str(reasoning_content)
        logger.info(f"[LLM思考过程] call_id={call_id}\n{reasoning_str}")
    else:
        logger.debug(f"[LLM思考过程] call_id={call_id}, 未找到思考过程")
    
    # ... 其余代码保持不变 ...
    
    # 构建响应快照（包含思考过程和最终答案）
    response_snapshot_parts = []
    if reasoning_content:
        response_snapshot_parts.append(f"[思考过程]\n{reasoning_content}")
    if response_text:
        response_snapshot_parts.append(f"[最终答案]\n{response_text}")
    response_snapshot = "\n\n".join(response_snapshot_parts) if response_snapshot_parts else None
    
    # ... 保存到数据库的代码 ...
```

**关键点**：
- **优先**从 `additional_kwargs` 提取 `reasoning_content`（Monkey Patch 方案的核心）
- 保留原有提取逻辑作为后备方案（兼容性）
- 思考过程单独记录为 `role="reasoning"` 的消息
- 在 `response_snapshot` 中包含思考过程和最终答案

### 3.4 步骤4：增强 langfuse_handler.py 中的记录逻辑

**文件路径**：`infrastructure/observability/langfuse_handler.py`

**修改内容**：
- 在 Langfuse Callback 中提取思考过程
- 通过 `metadata` 传递给 Langfuse

**实现方式**：

由于 Langfuse 的 `CallbackHandler` 是自动处理 LLM 调用的，我们需要通过以下方式传递思考过程：

**方案A：通过 metadata 传递**（推荐）

Langfuse 的 `CallbackHandler` 支持在回调中访问响应对象。我们需要创建一个自定义的 Callback Handler 来提取思考过程。

**实现代码**：

```python
# infrastructure/observability/langfuse_handler.py

# 在文件开头添加
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List

# 创建自定义的 Langfuse Callback Handler
class EnhancedLangfuseCallbackHandler(LangfuseCallbackHandler):
    """
    增强版 Langfuse Callback Handler
    支持提取和记录 reasoning_content
    """
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        LLM 调用结束回调
        
        提取 reasoning_content 并添加到 metadata
        """
        # 先调用父类方法
        super().on_llm_end(response, **kwargs)
        
        # 提取 reasoning_content
        reasoning_content = None
        try:
            generations = response.generations if hasattr(response, "generations") else []
            if generations and generations[0]:
                message = generations[0][0].message
                if hasattr(message, "additional_kwargs"):
                    additional_kwargs = getattr(message, "additional_kwargs", {}) or {}
                    reasoning_content = additional_kwargs.get("reasoning_content")
                    
                    if reasoning_content:
                        logger.debug(f"提取到 reasoning_content，长度: {len(reasoning_content)} 字符")
                        
                        # 获取当前的 generation（Langfuse SDK 3.x 可能需要通过其他方式访问）
                        # 注意：Langfuse SDK 的 API 可能因版本而异，需要根据实际版本调整
                        # 这里提供一个通用的方案：通过 metadata 传递
                        run_id = str(kwargs.get("run_id", ""))
                        if run_id:
                            # 尝试更新 generation 的 metadata
                            # 注意：这需要根据 Langfuse SDK 的实际 API 调整
                            try:
                                # 如果 Langfuse SDK 支持，可以通过以下方式更新
                                # 但需要确认 SDK 版本和 API
                                pass
                            except Exception as e:
                                logger.debug(f"更新 Langfuse metadata 失败: {e}")
        except Exception as e:
            logger.debug(f"提取 reasoning_content 失败: {e}", exc_info=True)
        
        # 如果无法通过 SDK API 直接更新，可以考虑：
        # 1. 通过 trace 的 metadata 传递（在 set_langfuse_trace_context 中）
        # 2. 通过自定义字段传递（如果 SDK 支持）
        # 3. 在 Langfuse UI 中通过搜索 metadata 来查看


def create_langfuse_handler(
    context: Optional[LlmLogContext] = None
) -> "LangfuseCallbackHandler":
    """
    创建 Langfuse Callback Handler（增强版）
    
    如果支持，返回增强版 Handler；否则返回标准 Handler
    """
    # ... 现有代码 ...
    
    # 创建 Handler
    if hasattr(LangfuseCallbackHandler, 'on_llm_end'):
        # 使用增强版 Handler
        handler = EnhancedLangfuseCallbackHandler(
            public_key=settings.LANGFUSE_PUBLIC_KEY,
            update_trace=True,
            trace_context=trace_context,
        )
    else:
        # 回退到标准 Handler
        handler = LangfuseCallbackHandler(
            public_key=settings.LANGFUSE_PUBLIC_KEY,
            update_trace=True,
            trace_context=trace_context,
        )
    
    return handler
```

**方案B：通过 trace metadata 传递**（备选）

如果无法在 generation 级别传递，可以在 trace 级别传递：

```python
def set_langfuse_trace_context(
    name: str,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    trace_id: Optional[str] = None,
    metadata: Optional[dict] = None,
    reasoning_content: Optional[str] = None,  # 新增参数
) -> Optional[str]:
    """
    设置 Langfuse trace 上下文（增强版）
    
    如果提供了 reasoning_content，会添加到 metadata 中
    """
    # ... 现有代码 ...
    
    if metadata is None:
        metadata = {}
    
    # 如果提供了 reasoning_content，添加到 metadata
    if reasoning_content:
        metadata["reasoning_content"] = reasoning_content
    
    # ... 其余代码保持不变 ...
```

**注意**：Langfuse SDK 的 API 可能因版本而异，需要根据实际使用的版本调整实现。

## 4. 实施步骤

### 4.1 步骤1：创建 reasoning_patch.py

1. 创建文件 `infrastructure/llm/reasoning_patch.py`
2. 实现 `apply_reasoning_patch()` 函数
3. 添加单元测试（可选）

### 4.2 步骤2：修改 client.py

1. 在文件开头导入 `apply_reasoning_patch`
2. 在导入 `ChatOpenAI` 之前应用补丁
3. 测试补丁是否成功应用

### 4.3 步骤3：增强 llm_logger.py

1. 修改 `on_llm_end` 方法
2. 优先从 `additional_kwargs` 提取 `reasoning_content`
3. 确保思考过程被记录到日志和数据库
4. 测试日志输出

### 4.4 步骤4：增强 langfuse_handler.py

1. 创建 `EnhancedLangfuseCallbackHandler` 类（如果需要）
2. 或通过 trace metadata 传递思考过程
3. 测试 Langfuse 中的显示

### 4.5 步骤5：测试验证

1. **单元测试**：测试补丁是否正确应用
2. **集成测试**：测试完整的 LLM 调用流程
3. **验证日志**：检查控制台日志是否显示思考过程
4. **验证数据库**：检查数据库是否保存了思考过程
5. **验证 Langfuse**：检查 Langfuse UI 中是否显示思考过程

## 5. 测试计划

### 5.1 单元测试

**测试文件**：`cursor_test/M6_test/test_reasoning_patch_integration.py`

**测试内容**：
1. 测试补丁是否正确应用
2. 测试 `reasoning_content` 是否出现在 `additional_kwargs` 中
3. 测试日志记录是否包含思考过程

### 5.2 集成测试

**测试场景**：
1. 完整的 LLM 调用流程（从 `get_llm` 到 `on_llm_end`）
2. 验证思考过程在日志中的显示
3. 验证思考过程在数据库中的存储
4. 验证思考过程在 Langfuse 中的记录

### 5.3 回归测试

确保现有功能不受影响：
1. 没有思考过程的模型调用仍然正常工作
2. 其他字段（如 `function_call`, `tool_calls`）仍然正常保留
3. 日志格式保持兼容

## 6. 风险评估与应对

### 6.1 技术风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| Monkey Patch 在 LangChain 升级后失效 | 中 | 锁定 LangChain 版本，升级时充分测试 |
| 补丁应用失败导致主流程中断 | 高 | 添加异常处理，失败时记录警告但不影响主流程 |
| Langfuse SDK API 变更 | 中 | 根据实际版本调整实现，提供备选方案 |

### 6.2 兼容性风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| 不同版本的 LangChain 行为不一致 | 中 | 在测试中覆盖不同版本 |
| Langfuse SDK 版本兼容性 | 低 | 使用稳定的 SDK 版本 |

### 6.3 性能风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| 思考过程提取增加延迟 | 低 | 提取逻辑简单，延迟可忽略 |
| 思考过程存储占用空间 | 中 | 使用截断存储，限制最大长度 |

## 7. 验收标准

### 7.1 功能验收

- [ ] Monkey Patch 成功应用，`reasoning_content` 出现在 `additional_kwargs` 中
- [ ] 控制台日志中显示思考过程（格式：`[LLM思考过程] call_id=xxx\n{reasoning_content}`）
- [ ] 数据库日志中保存思考过程（`response_snapshot` 或 `response_messages`）
- [ ] Langfuse UI 中能够查看思考过程（通过 metadata 或自定义字段）
- [ ] 所有现有功能正常工作（回归测试通过）

### 7.2 性能验收

- [ ] Monkey Patch 增加的延迟 < 1ms
- [ ] 思考过程提取不影响主流程性能
- [ ] 日志写入不影响主流程性能

### 7.3 质量验收

- [ ] 单元测试覆盖率 > 80%
- [ ] 集成测试通过
- [ ] 代码审查通过
- [ ] 文档完整

## 8. 后续优化方向

### 8.1 短期优化

1. **思考过程格式化**：对思考过程进行格式化，提高可读性
2. **思考过程分析**：分析思考过程的长度、复杂度等指标
3. **思考过程搜索**：在日志系统中支持思考过程的搜索和过滤

### 8.2 长期优化

1. **思考过程可视化**：在 Langfuse UI 中可视化思考过程
2. **思考过程优化**：基于思考过程分析，优化提示词和模型参数
3. **思考过程缓存**：对于相似的输入，缓存思考过程（如果模型支持）

## 9. 参考资料

- [002-LangChain思考过程提取可行性深度分析.md](./002-LangChain思考过程提取可行性深度分析.md)
- [001-思考过程调用方案.md](./001-思考过程调用方案.md)
- [测试代码](../cursor_test/M6_test/test_reasoning_content_extraction.py)
- [LangChain ChatOpenAI 文档](https://python.langchain.com/docs/integrations/chat/openai)
- [Langfuse 文档](https://langfuse.com/docs)

---

**文档生成时间**：2025-12-23  
**版本**：1.0  
**基于方案**：方案1 - Monkey Patch `_convert_dict_to_message`

