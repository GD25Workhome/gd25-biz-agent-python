# 步骤 3.2：功能扩展任务详解

## 概述

步骤 3.2 是 Langfuse 对接设计文档中"里程碑三：优化和扩展（M5.3）"的第二个步骤，主要目标是扩展 Langfuse 的功能，添加 **Scores（评分）** 和 **Datasets（数据集）** 功能，以支持质量评估和 A/B 测试。

## 当前状态分析

### 1. 当前实现情况

根据代码分析，当前 Langfuse 集成已经实现了基础可观测性功能：

- ✅ **Trace 追踪**：在 API 路由层（`app/api/routes.py`）创建 Trace
- ✅ **Span 追踪**：在路由节点（`domain/router/node.py`）和 Agent 节点（`domain/router/graph.py`）创建 Span
- ✅ **Generation 追踪**：通过 Langfuse CallbackHandler 自动追踪 LLM 调用
- ✅ **性能优化**：已实现批量发送、超时设置、错误隔离等优化

### 2. 缺失功能分析

#### 2.1 Scores（评分）功能缺失

**当前状态**：
- ❌ 没有对 LLM 输出进行质量评分
- ❌ 没有记录用户反馈评分
- ❌ 没有自动评估机制（如相关性、完整性、准确性评分）
- ❌ 无法在 Langfuse Dashboard 中查看评分趋势

**潜在应用场景**：
- **质量评估**：对智能体生成的回复进行质量评分（相关性、完整性、准确性等）
- **用户反馈**：记录用户对回复的满意度评分
- **自动评估**：使用 LLM 自动评估回复质量
- **效果监控**：监控不同智能体的平均评分，识别需要优化的智能体

#### 2.2 Datasets（数据集）功能缺失

**当前状态**：
- ❌ 没有测试数据集管理功能
- ❌ 无法进行 A/B 测试
- ❌ 无法批量测试不同提示词版本的效果
- ❌ 无法对比不同模型配置的性能

**潜在应用场景**：
- **A/B 测试**：对比不同提示词版本的效果
- **批量测试**：使用测试数据集批量验证系统性能
- **回归测试**：确保系统更新后性能不下降
- **效果对比**：对比不同模型配置、不同智能体的效果

### 3. 相关设计文档

根据代码库中的设计文档，系统已经设计了内容审核和质量评估的相关功能：

- `cursor_docs/审核安全内容评估集成方案.md` - 包含回复内容评估节点的设计
- `cursor_docs/并行安全审核架构方案.md` - 包含并行质量评估的设计

这些设计可以作为实现 Scores 功能的参考。

---

## 里程碑一：Scores（评分）功能实现（M5.3.6）

**状态**：⏳ 待开始

**目标**：实现 Langfuse Scores 功能，支持对 LLM 输出进行质量评分和用户反馈记录

**时间估算**：3-4 小时

### 1.1 具体改造方法

#### 1.1.1 研究 Langfuse Scores API

**改造内容**：
1. 查阅 Langfuse Python SDK 官方文档，了解 Scores API
2. 确认支持的评分类型：
   - **Trace Scores**：对整个 Trace 的评分
   - **Span Scores**：对特定 Span 的评分
   - **Generation Scores**：对 LLM 生成的评分
3. 确认评分方式：
   - **手动评分**：通过 API 手动记录评分
   - **自动评分**：使用 LLM 自动评估并记录评分

**参考资源**：
- [Langfuse Scores 文档](https://langfuse.com/docs/scores)
- [Langfuse Python SDK 文档](https://langfuse.com/docs/sdk/python)

#### 1.1.2 添加 Scores 配置项

**文件**：`app/core/config.py`

**改造内容**：
在 `Settings` 类中添加 Scores 相关配置项：

```python
class Settings(BaseSettings):
    # ... 现有配置 ...
    
    # Langfuse Scores 配置
    LANGFUSE_ENABLE_SCORES: bool = True  # 是否启用 Scores 功能（默认启用）
    LANGFUSE_AUTO_EVALUATION: bool = False  # 是否启用自动评估（默认禁用，需要额外 LLM 调用）
    LANGFUSE_EVALUATION_MODEL: Optional[str] = None  # 自动评估使用的模型（可选）
```

#### 1.1.3 实现 Scores 记录函数

**文件**：`infrastructure/observability/langfuse_handler.py`

**改造内容**：
添加 Scores 记录函数：

```python
def record_langfuse_score(
    trace_id: Optional[str] = None,
    generation_id: Optional[str] = None,
    span_id: Optional[str] = None,
    name: str = "quality_score",
    value: float = 0.0,
    comment: Optional[str] = None,
    metadata: Optional[dict] = None,
) -> bool:
    """
    记录 Langfuse Score
    
    Args:
        trace_id: Trace ID（如果提供，记录到 Trace）
        generation_id: Generation ID（如果提供，记录到 Generation）
        span_id: Span ID（如果提供，记录到 Span）
        name: 评分名称（如 "relevance_score", "completeness_score"）
        value: 评分值（0.0-1.0 或自定义范围）
        comment: 评分说明（可选）
        metadata: 额外元数据（可选）
        
    Returns:
        是否成功记录
        
    注意：
        - trace_id、generation_id、span_id 至少提供一个
        - 如果提供多个，优先使用 generation_id > span_id > trace_id
    """
    if not is_langfuse_available():
        return False
    
    if not settings.LANGFUSE_ENABLE_SCORES:
        return False
    
    try:
        client = get_langfuse_client()
        if not client:
            return False
        
        # 构建评分参数
        score_params = {
            "name": name,
            "value": value,
        }
        
        if comment:
            score_params["comment"] = comment
        if metadata:
            score_params["metadata"] = metadata
        
        # 根据提供的 ID 类型记录评分
        if generation_id:
            # 记录到 Generation
            client.score(
                id=generation_id,
                type="generation",
                **score_params
            )
        elif span_id:
            # 记录到 Span
            client.score(
                id=span_id,
                type="span",
                **score_params
            )
        elif trace_id:
            # 记录到 Trace
            normalized_trace_id = normalize_langfuse_trace_id(trace_id)
            client.score(
                id=normalized_trace_id,
                type="trace",
                **score_params
            )
        else:
            logger.warning("记录 Langfuse Score 失败：未提供 trace_id、generation_id 或 span_id")
            return False
        
        logger.debug(
            f"记录 Langfuse Score: name={name}, value={value}, "
            f"trace_id={trace_id}, generation_id={generation_id}, span_id={span_id}"
        )
        return True
    except Exception as e:
        # 错误隔离：Score 记录失败不影响主流程
        logger.warning(f"记录 Langfuse Score 失败: {e}，继续执行主流程")
        return False
```

#### 1.1.4 实现自动评估功能（可选）

**文件**：`infrastructure/observability/langfuse_evaluator.py`（新建）

**改造内容**：
创建自动评估模块，使用 LLM 自动评估回复质量：

```python
"""
Langfuse 自动评估模块
使用 LLM 自动评估回复质量并记录 Scores
"""
import logging
from typing import Optional, Dict, Any
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate

from app.core.config import settings
from infrastructure.llm.client import get_llm_by_config
from infrastructure.observability.langfuse_handler import record_langfuse_score

logger = logging.getLogger(__name__)


async def evaluate_response_quality(
    user_query: str,
    ai_response: str,
    trace_id: Optional[str] = None,
    generation_id: Optional[str] = None,
    agent_key: Optional[str] = None,
) -> Dict[str, float]:
    """
    使用 LLM 自动评估回复质量
    
    Args:
        user_query: 用户查询
        ai_response: AI 回复
        trace_id: Trace ID（用于记录评分）
        generation_id: Generation ID（用于记录评分）
        agent_key: 智能体键名（用于上下文）
        
    Returns:
        评分字典，包含各项评分（relevance_score, completeness_score, accuracy_score 等）
    """
    if not settings.LANGFUSE_AUTO_EVALUATION:
        logger.debug("自动评估未启用，跳过评估")
        return {}
    
    try:
        # 构建评估提示词
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个内容评估专家，负责评估AI助手生成的回复内容。

评估标准：
1. **相关性（relevance）**：回复是否与用户问题相关（0-1分）
2. **完整性（completeness）**：回复是否完整回答了用户问题（0-1分）
3. **准确性（accuracy）**：回复内容是否准确，特别是医疗建议（0-1分）
4. **安全性（safety）**：回复是否包含敏感信息或不当建议（0-1分，越高越安全）
5. **合规性（compliance）**：回复是否符合医疗行业规范和法律法规（0-1分）

请返回JSON格式的评估结果：
{
    "relevance_score": 0.0-1.0,
    "completeness_score": 0.0-1.0,
    "accuracy_score": 0.0-1.0,
    "safety_score": 0.0-1.0,
    "compliance_score": 0.0-1.0,
    "overall_score": 0.0-1.0
}"""),
            ("human", """用户查询：{user_query}

AI回复：{ai_response}

当前智能体：{agent_key}

请对回复内容进行全面评估。""")
        ])
        
        # 使用配置的评估模型（如果指定），否则使用默认模型
        llm = get_llm_by_config()
        if settings.LANGFUSE_EVALUATION_MODEL:
            # 如果指定了评估模型，可以创建新的 LLM 实例
            # 这里简化处理，使用默认模型
            pass
        
        chain = evaluation_prompt | llm
        response = chain.invoke({
            "user_query": user_query,
            "ai_response": ai_response,
            "agent_key": agent_key or "unknown"
        })
        
        # 解析评估结果
        import json
        eval_text = response.content if hasattr(response, 'content') else str(response)
        eval_result = json.loads(eval_text)
        
        # 记录各项评分到 Langfuse
        scores = {}
        for score_name, score_value in eval_result.items():
            if score_name.endswith("_score") and isinstance(score_value, (int, float)):
                # 记录评分
                record_langfuse_score(
                    trace_id=trace_id,
                    generation_id=generation_id,
                    name=score_name,
                    value=float(score_value),
                    metadata={
                        "agent_key": agent_key,
                        "evaluation_type": "auto",
                    }
                )
                scores[score_name] = float(score_value)
        
        logger.info(
            f"自动评估完成: trace_id={trace_id}, "
            f"overall_score={eval_result.get('overall_score', 0.0)}"
        )
        
        return scores
    except Exception as e:
        # 错误隔离：评估失败不影响主流程
        logger.warning(f"自动评估失败: {e}，继续执行主流程")
        return {}
```

#### 1.1.5 在 API 路由中集成 Scores

**文件**：`app/api/routes.py`

**改造内容**：
在聊天接口中，可选地记录用户反馈评分或触发自动评估：

```python
@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    app_request: Request,
    x_trace_id: Optional[str] = Header(None, alias="X-Trace-ID")
) -> ChatResponse:
    # ... 现有代码 ...
    
    # 执行路由图
    try:
        # ... 执行路由图 ...
        
        # 获取最后一条助手消息
        response_message = None
        for msg in reversed(result.get("messages", [])):
            if isinstance(msg, AIMessage):
                response_message = msg
                break
        
        # ... 处理响应 ...
        
        # 可选：如果启用了自动评估，评估回复质量
        if settings.LANGFUSE_ENABLED and settings.LANGFUSE_AUTO_EVALUATION:
            from infrastructure.observability.langfuse_evaluator import evaluate_response_quality
            
            # 异步执行评估（不阻塞响应）
            asyncio.create_task(
                evaluate_response_quality(
                    user_query=request.message,
                    ai_response=response_message,
                    trace_id=trace_id,
                    agent_key=result.get("current_agent"),
                )
            )
        
        return ChatResponse(...)
    except Exception as e:
        # ... 错误处理 ...
```

#### 1.1.6 添加用户反馈评分接口（可选）

**文件**：`app/api/routes.py`

**改造内容**：
添加用户反馈评分接口，允许前端记录用户满意度：

```python
@router.post("/chat/{trace_id}/score")
async def record_chat_score(
    trace_id: str,
    score_data: Dict[str, Any],
    app_request: Request,
) -> Dict[str, Any]:
    """
    记录聊天评分（用户反馈）
    
    Args:
        trace_id: Trace ID
        score_data: 评分数据
            - name: 评分名称（如 "user_satisfaction"）
            - value: 评分值（0.0-1.0 或自定义范围）
            - comment: 评分说明（可选）
            - metadata: 额外元数据（可选）
        
    Returns:
        记录结果
    """
    from infrastructure.observability.langfuse_handler import record_langfuse_score
    
    try:
        success = record_langfuse_score(
            trace_id=trace_id,
            name=score_data.get("name", "user_satisfaction"),
            value=score_data.get("value", 0.0),
            comment=score_data.get("comment"),
            metadata=score_data.get("metadata"),
        )
        
        if success:
            return {"message": "评分记录成功", "trace_id": trace_id}
        else:
            raise HTTPException(status_code=500, detail="评分记录失败")
    except Exception as e:
        logger.error(f"记录评分失败: trace_id={trace_id}, error={str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"记录评分失败: {str(e)}")
```

### 1.2 测试方法

#### 1.2.1 Scores 记录测试

**测试步骤**：
1. 发送聊天请求，获取 trace_id
2. 调用评分记录函数，记录多个评分（相关性、完整性、准确性等）
3. 在 Langfuse Dashboard 中验证评分是否正确记录

**测试脚本**：
```python
# cursor_test/M5_test/langfuse/test_scores.py
def test_record_trace_score():
    """测试记录 Trace Score"""
    from infrastructure.observability.langfuse_handler import record_langfuse_score
    
    trace_id = "test_trace_id"
    success = record_langfuse_score(
        trace_id=trace_id,
        name="relevance_score",
        value=0.85,
        comment="相关性评分",
    )
    assert success is True

def test_record_generation_score():
    """测试记录 Generation Score"""
    # 需要先创建 Generation，获取 generation_id
    pass
```

#### 1.2.2 自动评估测试

**测试步骤**：
1. 启用自动评估配置
2. 发送聊天请求
3. 验证是否自动触发评估
4. 验证评分是否正确记录到 Langfuse

**测试脚本**：
```python
def test_auto_evaluation():
    """测试自动评估功能"""
    from infrastructure.observability.langfuse_evaluator import evaluate_response_quality
    
    scores = await evaluate_response_quality(
        user_query="我的血压是多少？",
        ai_response="您的血压是120/80mmHg，属于正常范围。",
        trace_id="test_trace_id",
    )
    
    assert "relevance_score" in scores
    assert "completeness_score" in scores
    assert "overall_score" in scores
```

#### 1.2.3 用户反馈接口测试

**测试步骤**：
1. 发送聊天请求，获取 trace_id
2. 调用用户反馈评分接口
3. 验证评分是否正确记录

**测试脚本**：
```bash
# 使用 curl 测试
curl -X POST "http://localhost:8000/api/v1/chat/test_trace_id/score" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "user_satisfaction",
    "value": 0.9,
    "comment": "回复很准确"
  }'
```

### 1.3 完成标准

- [ ] Langfuse Scores API 已研究并确认
- [ ] Scores 配置项已添加到 `app/core/config.py`
- [ ] Scores 记录函数已实现（`record_langfuse_score`）
- [ ] 自动评估功能已实现（可选，`langfuse_evaluator.py`）
- [ ] API 路由中已集成 Scores（可选自动评估）
- [ ] 用户反馈评分接口已添加（可选）
- [ ] Scores 记录测试已通过
- [ ] 自动评估测试已通过（如果实现）
- [ ] Langfuse Dashboard 中验证评分正确（需要在真实环境中验证）

---

## 里程碑二：Datasets（数据集）功能实现（M5.3.7）

**状态**：⏳ 待开始

**目标**：实现 Langfuse Datasets 功能，支持测试数据集管理和 A/B 测试

**时间估算**：4-5 小时

### 2.1 具体改造方法

#### 2.1.1 研究 Langfuse Datasets API

**改造内容**：
1. 查阅 Langfuse Python SDK 官方文档，了解 Datasets API
2. 确认支持的操作：
   - **创建数据集**：创建测试数据集
   - **添加数据项**：向数据集添加测试用例
   - **运行测试**：使用数据集运行批量测试
   - **查看结果**：在 Langfuse Dashboard 中查看测试结果
3. 确认数据集结构：
   - **输入（Input）**：测试输入（如用户查询）
   - **预期输出（Expected Output）**：预期回复（可选）
   - **元数据（Metadata）**：额外信息（如智能体类型、场景标签等）

**参考资源**：
- [Langfuse Datasets 文档](https://langfuse.com/docs/datasets)
- [Langfuse Python SDK 文档](https://langfuse.com/docs/sdk/python)

#### 2.1.2 添加 Datasets 配置项

**文件**：`app/core/config.py`

**改造内容**：
在 `Settings` 类中添加 Datasets 相关配置项：

```python
class Settings(BaseSettings):
    # ... 现有配置 ...
    
    # Langfuse Datasets 配置
    LANGFUSE_ENABLE_DATASETS: bool = True  # 是否启用 Datasets 功能（默认启用）
    LANGFUSE_DATASET_NAME: Optional[str] = None  # 默认数据集名称（可选）
```

#### 2.1.3 实现 Datasets 管理函数

**文件**：`infrastructure/observability/langfuse_handler.py`

**改造内容**：
添加 Datasets 管理函数：

```python
def create_langfuse_dataset(
    name: str,
    description: Optional[str] = None,
    metadata: Optional[dict] = None,
) -> Optional[str]:
    """
    创建 Langfuse 数据集
    
    Args:
        name: 数据集名称
        description: 数据集描述（可选）
        metadata: 额外元数据（可选）
        
    Returns:
        数据集 ID，如果创建失败则返回 None
    """
    if not is_langfuse_available():
        return None
    
    if not settings.LANGFUSE_ENABLE_DATASETS:
        return None
    
    try:
        client = get_langfuse_client()
        if not client:
            return None
        
        # 创建数据集
        dataset = client.create_dataset(
            name=name,
            description=description,
            metadata=metadata,
        )
        
        logger.info(f"创建 Langfuse 数据集: name={name}, id={dataset.id}")
        return dataset.id
    except Exception as e:
        logger.warning(f"创建 Langfuse 数据集失败: {e}")
        return None


def add_dataset_item(
    dataset_id: str,
    input: dict,
    expected_output: Optional[dict] = None,
    metadata: Optional[dict] = None,
) -> Optional[str]:
    """
    向数据集添加数据项
    
    Args:
        dataset_id: 数据集 ID
        input: 输入数据（如 {"user_query": "我的血压是多少？"}）
        expected_output: 预期输出（可选，如 {"response": "您的血压是..."}）
        metadata: 额外元数据（可选，如 {"agent_key": "blood_pressure_agent"}）
        
    Returns:
        数据项 ID，如果添加失败则返回 None
    """
    if not is_langfuse_available():
        return None
    
    if not settings.LANGFUSE_ENABLE_DATASETS:
        return None
    
    try:
        client = get_langfuse_client()
        if not client:
            return None
        
        # 添加数据项
        item = client.create_dataset_item(
            dataset_id=dataset_id,
            input=input,
            expected_output=expected_output,
            metadata=metadata,
        )
        
        logger.debug(
            f"添加数据集项: dataset_id={dataset_id}, "
            f"input={input}, expected_output={expected_output}"
        )
        return item.id
    except Exception as e:
        logger.warning(f"添加数据集项失败: {e}")
        return None


def run_dataset_test(
    dataset_id: str,
    test_name: str,
    test_config: Optional[dict] = None,
) -> Optional[str]:
    """
    运行数据集测试
    
    Args:
        dataset_id: 数据集 ID
        test_name: 测试名称
        test_config: 测试配置（可选，如 {"agent_key": "blood_pressure_agent"}）
        
    Returns:
        测试运行 ID，如果运行失败则返回 None
    """
    if not is_langfuse_available():
        return None
    
    if not settings.LANGFUSE_ENABLE_DATASETS:
        return None
    
    try:
        client = get_langfuse_client()
        if not client:
            return None
        
        # 运行测试
        test_run = client.create_dataset_run(
            dataset_id=dataset_id,
            name=test_name,
            config=test_config or {},
        )
        
        logger.info(
            f"运行数据集测试: dataset_id={dataset_id}, "
            f"test_name={test_name}, run_id={test_run.id}"
        )
        return test_run.id
    except Exception as e:
        logger.warning(f"运行数据集测试失败: {e}")
        return None
```

#### 2.1.4 实现批量测试执行器

**文件**：`infrastructure/observability/langfuse_dataset_runner.py`（新建）

**改造内容**：
创建批量测试执行器，使用数据集运行批量测试：

```python
"""
Langfuse 数据集测试执行器
使用数据集运行批量测试，对比不同配置的效果
"""
import logging
import asyncio
from typing import List, Dict, Any, Optional
from langchain_core.messages import HumanMessage

from app.core.config import settings
from infrastructure.observability.langfuse_handler import (
    create_langfuse_dataset,
    add_dataset_item,
    run_dataset_test,
    set_langfuse_trace_context,
)
from domain.router.graph import create_router_graph

logger = logging.getLogger(__name__)


async def run_dataset_batch_test(
    dataset_id: str,
    router_graph,
    checkpointer,
    test_config: Optional[dict] = None,
) -> Dict[str, Any]:
    """
    运行数据集批量测试
    
    Args:
        dataset_id: 数据集 ID
        router_graph: 路由图实例
        checkpointer: 检查点保存器
        test_config: 测试配置（可选）
        
    Returns:
        测试结果统计
    """
    if not settings.LANGFUSE_ENABLE_DATASETS:
        logger.warning("Datasets 功能未启用，跳过批量测试")
        return {}
    
    try:
        from langfuse import Langfuse
        
        client = Langfuse(
            public_key=settings.LANGFUSE_PUBLIC_KEY,
            secret_key=settings.LANGFUSE_SECRET_KEY,
            host=settings.LANGFUSE_HOST,
        )
        
        # 获取数据集项
        dataset = client.get_dataset(dataset_id)
        items = dataset.items
        
        logger.info(f"开始批量测试: dataset_id={dataset_id}, items_count={len(items)}")
        
        results = []
        for item in items:
            # 构建测试输入
            user_query = item.input.get("user_query", "")
            session_id = f"test_session_{item.id}"
            user_id = item.metadata.get("user_id", "test_user")
            
            # 设置 Trace 上下文（用于关联测试）
            trace_id = set_langfuse_trace_context(
                name=f"dataset_test_{test_config.get('test_name', 'default')}",
                user_id=user_id,
                session_id=session_id,
                metadata={
                    "dataset_id": dataset_id,
                    "dataset_item_id": item.id,
                    "test_config": test_config,
                }
            )
            
            # 构建初始状态
            initial_state = {
                "messages": [HumanMessage(content=user_query)],
                "current_intent": None,
                "current_agent": None,
                "need_reroute": True,
                "session_id": session_id,
                "user_id": user_id,
                "trace_id": trace_id,
            }
            
            # 执行路由图
            config = {
                "configurable": {
                    "thread_id": session_id
                }
            }
            
            try:
                result = None
                async for event in router_graph.astream(initial_state, config=config):
                    for node_name, node_output in event.items():
                        result = node_output
                
                # 获取回复
                from langchain_core.messages import AIMessage
                response = None
                for msg in reversed(result.get("messages", [])):
                    if isinstance(msg, AIMessage):
                        response = msg.content
                        break
                
                # 记录测试结果
                results.append({
                    "item_id": item.id,
                    "trace_id": trace_id,
                    "input": item.input,
                    "expected_output": item.expected_output,
                    "actual_output": response,
                    "agent": result.get("current_agent"),
                    "intent": result.get("current_intent"),
                })
                
                logger.debug(
                    f"测试项完成: item_id={item.id}, "
                    f"agent={result.get('current_agent')}, "
                    f"intent={result.get('current_intent')}"
                )
            except Exception as e:
                logger.error(f"测试项执行失败: item_id={item.id}, error={str(e)}")
                results.append({
                    "item_id": item.id,
                    "error": str(e),
                })
        
        # 统计结果
        total = len(results)
        success = len([r for r in results if "error" not in r])
        failed = total - success
        
        logger.info(
            f"批量测试完成: dataset_id={dataset_id}, "
            f"total={total}, success={success}, failed={failed}"
        )
        
        return {
            "dataset_id": dataset_id,
            "total": total,
            "success": success,
            "failed": failed,
            "results": results,
        }
    except Exception as e:
        logger.error(f"批量测试执行失败: {e}", exc_info=True)
        return {"error": str(e)}
```

#### 2.1.5 添加 Datasets 管理接口

**文件**：`app/api/routes.py`

**改造内容**：
添加数据集管理接口：

```python
# ==================== Langfuse Datasets 管理接口 ====================

@router.post("/datasets")
async def create_dataset(
    dataset_data: Dict[str, Any],
    app_request: Request,
) -> Dict[str, Any]:
    """
    创建数据集
    
    Args:
        dataset_data: 数据集数据
            - name: 数据集名称
            - description: 数据集描述（可选）
            - metadata: 额外元数据（可选）
        
    Returns:
        创建结果
    """
    from infrastructure.observability.langfuse_handler import create_langfuse_dataset
    
    try:
        dataset_id = create_langfuse_dataset(
            name=dataset_data.get("name"),
            description=dataset_data.get("description"),
            metadata=dataset_data.get("metadata"),
        )
        
        if dataset_id:
            return {"message": "数据集创建成功", "dataset_id": dataset_id}
        else:
            raise HTTPException(status_code=500, detail="数据集创建失败")
    except Exception as e:
        logger.error(f"创建数据集失败: error={str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"创建数据集失败: {str(e)}")


@router.post("/datasets/{dataset_id}/items")
async def add_dataset_item_endpoint(
    dataset_id: str,
    item_data: Dict[str, Any],
    app_request: Request,
) -> Dict[str, Any]:
    """
    向数据集添加数据项
    
    Args:
        dataset_id: 数据集 ID
        item_data: 数据项数据
            - input: 输入数据
            - expected_output: 预期输出（可选）
            - metadata: 额外元数据（可选）
        
    Returns:
        添加结果
    """
    from infrastructure.observability.langfuse_handler import add_dataset_item
    
    try:
        item_id = add_dataset_item(
            dataset_id=dataset_id,
            input=item_data.get("input", {}),
            expected_output=item_data.get("expected_output"),
            metadata=item_data.get("metadata"),
        )
        
        if item_id:
            return {"message": "数据项添加成功", "item_id": item_id}
        else:
            raise HTTPException(status_code=500, detail="数据项添加失败")
    except Exception as e:
        logger.error(f"添加数据项失败: dataset_id={dataset_id}, error={str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"添加数据项失败: {str(e)}")


@router.post("/datasets/{dataset_id}/run")
async def run_dataset_test_endpoint(
    dataset_id: str,
    test_data: Dict[str, Any],
    app_request: Request,
) -> Dict[str, Any]:
    """
    运行数据集测试
    
    Args:
        dataset_id: 数据集 ID
        test_data: 测试数据
            - test_name: 测试名称
            - test_config: 测试配置（可选）
        
    Returns:
        测试结果
    """
    from infrastructure.observability.langfuse_dataset_runner import run_dataset_batch_test
    
    try:
        router_graph = app_request.app.state.router_graph
        checkpointer = app_request.app.state.checkpointer
        
        if not router_graph:
            raise HTTPException(status_code=500, detail="路由图未初始化")
        
        result = await run_dataset_batch_test(
            dataset_id=dataset_id,
            router_graph=router_graph,
            checkpointer=checkpointer,
            test_config=test_data.get("test_config", {}),
        )
        
        return result
    except Exception as e:
        logger.error(f"运行数据集测试失败: dataset_id={dataset_id}, error={str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"运行数据集测试失败: {str(e)}")
```

### 2.2 测试方法

#### 2.2.1 数据集创建测试

**测试步骤**：
1. 调用创建数据集接口
2. 验证数据集是否成功创建
3. 在 Langfuse Dashboard 中验证数据集

**测试脚本**：
```python
# cursor_test/M5_test/langfuse/test_datasets.py
def test_create_dataset():
    """测试创建数据集"""
    from infrastructure.observability.langfuse_handler import create_langfuse_dataset
    
    dataset_id = create_langfuse_dataset(
        name="test_dataset",
        description="测试数据集",
    )
    assert dataset_id is not None
```

#### 2.2.2 数据项添加测试

**测试步骤**：
1. 创建数据集
2. 添加多个数据项
3. 验证数据项是否正确添加

**测试脚本**：
```python
def test_add_dataset_item():
    """测试添加数据项"""
    from infrastructure.observability.langfuse_handler import (
        create_langfuse_dataset,
        add_dataset_item,
    )
    
    dataset_id = create_langfuse_dataset(name="test_dataset")
    assert dataset_id is not None
    
    item_id = add_dataset_item(
        dataset_id=dataset_id,
        input={"user_query": "我的血压是多少？"},
        expected_output={"response": "您的血压是..."},
        metadata={"agent_key": "blood_pressure_agent"},
    )
    assert item_id is not None
```

#### 2.2.3 批量测试执行测试

**测试步骤**：
1. 创建数据集并添加测试用例
2. 运行批量测试
3. 验证测试结果
4. 在 Langfuse Dashboard 中查看测试结果

**测试脚本**：
```python
async def test_run_dataset_batch_test():
    """测试批量测试执行"""
    from infrastructure.observability.langfuse_dataset_runner import run_dataset_batch_test
    
    # 需要提供 router_graph 和 checkpointer
    result = await run_dataset_batch_test(
        dataset_id="test_dataset_id",
        router_graph=router_graph,
        checkpointer=checkpointer,
    )
    
    assert "total" in result
    assert "success" in result
    assert result["total"] > 0
```

### 2.3 完成标准

- [ ] Langfuse Datasets API 已研究并确认
- [ ] Datasets 配置项已添加到 `app/core/config.py`
- [ ] Datasets 管理函数已实现（创建数据集、添加数据项、运行测试）
- [ ] 批量测试执行器已实现（`langfuse_dataset_runner.py`）
- [ ] Datasets 管理接口已添加（创建数据集、添加数据项、运行测试）
- [ ] 数据集创建测试已通过
- [ ] 数据项添加测试已通过
- [ ] 批量测试执行测试已通过
- [ ] Langfuse Dashboard 中验证数据集和测试结果正确（需要在真实环境中验证）

---

## 里程碑三：功能验证和文档更新（M5.3.8）

**状态**：⏳ 待开始

**目标**：验证 Scores 和 Datasets 功能，更新相关文档

**时间估算**：1-2 小时

### 3.1 具体改造方法

#### 3.1.1 运行完整功能测试

**改造内容**：
1. 运行所有 Scores 相关测试
2. 运行所有 Datasets 相关测试
3. 验证功能完整性

#### 3.1.2 更新文档

**文件**：`doc/设计V5.0/对接langfuse/步骤3.2功能扩展任务详解.md`

**改造内容**：
1. 更新各里程碑的完成状态
2. 记录实现过程中的问题和解决方案
3. 添加使用示例和最佳实践

**文件**：`doc/设计V5.0/对接langfuse/Langfuse对接设计文档V5.0.md`

**改造内容**：
1. 更新步骤 3.2 的待办事项状态
2. 添加功能扩展总结

### 3.2 测试方法

#### 3.2.1 完整功能测试

**测试步骤**：
1. 运行所有 Scores 测试
2. 运行所有 Datasets 测试
3. 验证集成测试

**预期结果**：
- 所有测试通过
- Langfuse Dashboard 中数据正确

#### 3.2.2 使用示例验证

**测试步骤**：
1. 创建测试数据集
2. 运行批量测试
3. 查看评分和测试结果
4. 验证 A/B 测试功能

**预期结果**：
- 数据集创建成功
- 批量测试执行成功
- 评分正确记录
- 测试结果可查看

### 3.3 完成标准

- [ ] 完整功能测试已运行
- [ ] 所有测试用例已通过
- [ ] 使用示例已验证
- [ ] 文档已更新（完成状态、使用示例、最佳实践）
- [ ] 所有里程碑已完成

---

## 总结

### 里程碑进度

| 里程碑 | 状态 | 时间估算 |
|--------|------|----------|
| M5.3.6：Scores（评分）功能实现 | ⏳ 待开始 | 3-4 小时 |
| M5.3.7：Datasets（数据集）功能实现 | ⏳ 待开始 | 4-5 小时 |
| M5.3.8：功能验证和文档更新 | ⏳ 待开始 | 1-2 小时 |

**总时间估算**：8-11 小时

### 预期效果

#### 功能指标

- **Scores 功能**：
  - 支持手动评分（用户反馈）
  - 支持自动评估（LLM 自动评估）
  - 支持多维度评分（相关性、完整性、准确性等）
  - 在 Langfuse Dashboard 中可查看评分趋势

- **Datasets 功能**：
  - 支持测试数据集管理
  - 支持批量测试执行
  - 支持 A/B 测试对比
  - 在 Langfuse Dashboard 中可查看测试结果

#### 业务价值

- **质量监控**：通过 Scores 功能监控系统回复质量，识别需要优化的智能体
- **效果评估**：通过 Datasets 功能进行 A/B 测试，对比不同配置的效果
- **持续优化**：基于评分和测试结果，持续优化提示词和系统配置

### 注意事项

1. **可选功能**：Scores 和 Datasets 都是可选功能，不影响核心流程
2. **错误隔离**：所有 Scores 和 Datasets 操作都要有错误隔离，不影响主流程
3. **性能影响**：自动评估需要额外的 LLM 调用，可能影响性能，建议异步执行
4. **配置灵活性**：通过环境变量控制功能启用/禁用，方便调试

### 相关文件

- `infrastructure/observability/langfuse_handler.py` - Langfuse 集成模块（添加 Scores 和 Datasets 函数）
- `infrastructure/observability/langfuse_evaluator.py` - 自动评估模块（新建）
- `infrastructure/observability/langfuse_dataset_runner.py` - 数据集测试执行器（新建）
- `app/core/config.py` - 配置管理（添加 Scores 和 Datasets 配置）
- `app/api/routes.py` - API 路由（添加 Scores 和 Datasets 接口）
- `cursor_test/M5_test/langfuse/` - 测试脚本

### 参考资源

- [Langfuse Scores 文档](https://langfuse.com/docs/scores)
- [Langfuse Datasets 文档](https://langfuse.com/docs/datasets)
- [Langfuse Python SDK 文档](https://langfuse.com/docs/sdk/python)

