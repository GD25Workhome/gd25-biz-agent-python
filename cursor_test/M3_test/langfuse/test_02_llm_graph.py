"""
æµ‹è¯• 2ï¼šå¸¦ LLM è°ƒç”¨çš„ç®€å• LangGraph

ç›®çš„ï¼š
- éªŒè¯ LLM è°ƒç”¨è¢«æ­£ç¡®è¿½è¸ª
- éªŒè¯ Generation ç±»å‹çš„ Span
- éªŒè¯ tokens ä½¿ç”¨æƒ…å†µ
"""
import os
import sys
from typing import TypedDict, List
from pathlib import Path
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langfuse import Langfuse
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, project_root)

# åŠ è½½ .env æ–‡ä»¶
env_path = Path(project_root) / ".env"
if env_path.exists():
    load_dotenv(env_path)
else:
    # å°è¯•åŠ è½½ .env example
    env_example_path = Path(project_root) / ".env example"
    if env_example_path.exists():
        load_dotenv(env_example_path)

# å¯¼å…¥é¡¹ç›®çš„ LLM å®¢æˆ·ç«¯
from infrastructure.llm.client import get_llm


# å®šä¹‰å›¾çŠ¶æ€
class LLMGraphState(TypedDict):
    """åŒ…å«æ¶ˆæ¯å’Œ LLM å“åº”çš„å›¾çŠ¶æ€"""
    messages: List[str]
    llm_response: str
    step_count: int


# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨ Langfuse å®¢æˆ·ç«¯å’Œ traceï¼ˆåœ¨æµ‹è¯•ä¸­è®¾ç½®ï¼‰
_langfuse_client = None
_current_trace = None


def prepare_llm_node(state: LLMGraphState) -> LLMGraphState:
    """
    èŠ‚ç‚¹ï¼šå‡†å¤‡ LLM è°ƒç”¨
    å°†æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
    """
    if _langfuse_client:
        span = _langfuse_client.start_span(name="prepare_llm_node", input=state)
    
    messages = state.get("messages", [])
    # å°†å­—ç¬¦ä¸²æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼ï¼ˆç”¨äºåç»­ LLM è°ƒç”¨ï¼‰
    langchain_messages = [HumanMessage(content=msg) for msg in messages]
    
    result = {
        **state,
        "langchain_messages": langchain_messages,
        "step_count": state.get("step_count", 0) + 1
    }
    
    if _langfuse_client:
        span.update(output={"prepared_messages_count": len(langchain_messages)})
        span.end()
    
    return result


def call_llm_node(state: LLMGraphState) -> LLMGraphState:
    """
    èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM
    ä½¿ç”¨é¡¹ç›®çš„ get_llm() å‡½æ•°è°ƒç”¨ LLM
    ä½¿ç”¨ Langfuse æ‰‹åŠ¨è¿½è¸ª LLM è°ƒç”¨
    """
    if _langfuse_client and _current_trace:
        span = _langfuse_client.start_span(
            name="call_llm_node",
            input={"messages_count": len(state.get("langchain_messages", []))}
        )
    
    # è·å– LangChain æ¶ˆæ¯
    langchain_messages = state.get("langchain_messages", [])
    if not langchain_messages:
        # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ¶ˆæ¯
        langchain_messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")]
    
    # å‡†å¤‡è¾“å…¥æ–‡æœ¬ï¼ˆç”¨äº Langfuse Generationï¼‰
    input_text = "\n".join([msg.content if hasattr(msg, 'content') else str(msg) for msg in langchain_messages])
    
    # åˆ›å»º Generation span æ¥è¿½è¸ª LLM è°ƒç”¨
    generation = None
    if _langfuse_client and _current_trace:
        generation = _current_trace.start_generation(
            name="llm_call",
            model=os.getenv("LLM_MODEL", "deepseek-chat"),
            input=input_text,
            metadata={
                "node": "call_llm_node",
                "messages_count": len(langchain_messages)
            }
        )
    
    # è·å– LLM å®ä¾‹
    llm = get_llm(
        temperature=0.7,
        enable_logging=False  # ç¦ç”¨é¡¹ç›®çš„æ—¥å¿—ï¼Œä½¿ç”¨ Langfuse è¿½è¸ª
    )
    
    # è°ƒç”¨ LLM
    try:
        response = llm.invoke(langchain_messages)
        llm_response_text = response.content if hasattr(response, 'content') else str(response)
        
        # æ›´æ–° Generation span
        if generation:
            # å°è¯•è·å– usage ä¿¡æ¯ï¼ˆå¦‚æœ LLM è¿”å›äº†ï¼‰
            usage = None
            if hasattr(response, 'response_metadata') and response.response_metadata:
                usage_info = response.response_metadata.get('token_usage', {})
                if usage_info:
                    usage = {
                        "prompt_tokens": usage_info.get("prompt_tokens", 0),
                        "completion_tokens": usage_info.get("completion_tokens", 0),
                        "total_tokens": usage_info.get("total_tokens", 0)
                    }
            
            generation.update(
                output=llm_response_text,
                usage=usage
            )
            generation.end()
            
    except Exception as e:
        llm_response_text = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
        if generation:
            generation.update(
                output=llm_response_text,
                level="ERROR",
                status_message=str(e)
            )
            generation.end()
        if _langfuse_client and _current_trace:
            span.update(status_message=f"Error: {str(e)}", level="ERROR")
    
    result = {
        **state,
        "llm_response": llm_response_text,
        "step_count": state.get("step_count", 0) + 1
    }
    
    if _langfuse_client and _current_trace:
        span.update(output={"response_length": len(llm_response_text)})
        span.end()
    
    return result


def process_response_node(state: LLMGraphState) -> LLMGraphState:
    """
    èŠ‚ç‚¹ï¼šå¤„ç† LLM å“åº”
    å°†å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
    """
    if _langfuse_client:
        span = _langfuse_client.start_span(name="process_response_node", input=state)
    
    messages = state.get("messages", [])
    llm_response = state.get("llm_response", "")
    
    # å°† LLM å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    messages.append(f"LLMå›å¤: {llm_response}")
    
    result = {
        **state,
        "messages": messages,
        "step_count": state.get("step_count", 0) + 1
    }
    
    if _langfuse_client:
        span.update(output={"final_messages_count": len(messages)})
        span.end()
    
    return result


def create_llm_graph():
    """
    åˆ›å»ºåŒ…å« LLM è°ƒç”¨çš„ LangGraph
    æµç¨‹ï¼šprepare -> call_llm -> process -> END
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(LLMGraphState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("prepare_llm", prepare_llm_node)
    workflow.add_node("call_llm", call_llm_node)
    workflow.add_node("process_response", process_response_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("prepare_llm")
    
    # æ·»åŠ è¾¹ï¼šprepare -> call_llm -> process -> END
    workflow.add_edge("prepare_llm", "call_llm")
    workflow.add_edge("call_llm", "process_response")
    workflow.add_edge("process_response", END)
    
    # ç¼–è¯‘å›¾
    return workflow.compile()


def test_llm_graph_with_langfuse():
    """
    æµ‹è¯•åŒ…å« LLM è°ƒç”¨çš„ LangGraphï¼Œä½¿ç”¨ Langfuse è¿½è¸ª
    """
    print("=" * 60)
    print("æµ‹è¯• 2ï¼šå¸¦ LLM è°ƒç”¨çš„ç®€å• LangGraph")
    print("=" * 60)
    
    # åˆå§‹åŒ– Langfuseï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰
    langfuse_public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
    langfuse_secret_key = os.getenv("LANGFUSE_SECRET_KEY")
    langfuse_host = os.getenv("LANGFUSE_BASE_URL") or os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
    
    if not langfuse_public_key or not langfuse_secret_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° Langfuse å‡­æ®")
        print("   è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­é…ç½®äº†ï¼š")
        print("   - LANGFUSE_PUBLIC_KEY")
        print("   - LANGFUSE_SECRET_KEY")
        print("   - LANGFUSE_BASE_URL (å¯é€‰)")
        return False
    
    # æ£€æŸ¥ LLM é…ç½®
    llm_api_key = os.getenv("OPENAI_API_KEY")
    if not llm_api_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° LLM API Key")
        print("   è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­é…ç½®äº†ï¼š")
        print("   - OPENAI_API_KEY")
        return False
    
    print(f"âœ… Langfuse é…ç½®æ£€æŸ¥é€šè¿‡")
    print(f"   - Public Key: {langfuse_public_key[:20]}...")
    print(f"   - Host: {langfuse_host}")
    print(f"âœ… LLM é…ç½®æ£€æŸ¥é€šè¿‡")
    print(f"   - API Key: {llm_api_key[:20]}...")
    
    # åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯
    global _langfuse_client, _current_trace
    _langfuse_client = Langfuse(
        public_key=langfuse_public_key,
        secret_key=langfuse_secret_key,
        host=langfuse_host
    )
    
    # åˆ›å»º Traceï¼ˆç”¨äºåœ¨ Dashboard ä¸­è¯†åˆ«ï¼‰
    _current_trace = _langfuse_client.start_span(
        name="test_02_llm_graph",
        metadata={
            "test_name": "æµ‹è¯•2ï¼šå¸¦LLMè°ƒç”¨çš„LangGraph",
            "description": "éªŒè¯LLMè°ƒç”¨è¢«æ­£ç¡®è¿½è¸ªï¼ŒåŒ…æ‹¬Generation Spanå’Œtokensç»Ÿè®¡",
            "nodes": ["prepare_llm", "call_llm", "process_response"]
        }
    )
    
    print(f"âœ… åˆ›å»º Trace: {_current_trace.id}")
    
    # åˆ›å»ºå›¾
    graph = create_llm_graph()
    print("âœ… åˆ›å»º LangGraphï¼ˆåŒ…å« LLM è°ƒç”¨ï¼‰")
    
    # å‡†å¤‡åˆå§‹çŠ¶æ€
    initial_state: LLMGraphState = {
        "messages": ["ç”¨æˆ·æ¶ˆæ¯: ä½ å¥½"],
        "llm_response": "",
        "step_count": 0
    }
    
    print(f"ğŸ“¥ åˆå§‹çŠ¶æ€: {initial_state}")
    
    # åœ¨ trace ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œå›¾
    _current_trace.update(input=initial_state)
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œå›¾ï¼ˆå°†è°ƒç”¨ LLMï¼‰...")
    
    try:
        result = graph.invoke(initial_state)
        _current_trace.update(output=result)
        
        print(f"âœ… æ‰§è¡Œå®Œæˆ")
        print(f"ğŸ“¤ æœ€ç»ˆçŠ¶æ€:")
        print(f"   - æ¶ˆæ¯æ•°é‡: {len(result.get('messages', []))}")
        print(f"   - LLM å“åº”: {result.get('llm_response', '')[:100]}...")
        print(f"   - æ‰§è¡Œæ­¥æ•°: {result.get('step_count', 0)}")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        _current_trace.update(status_message=f"Error: {str(e)}", level="ERROR")
        raise
    
    finally:
        # ç»“æŸ trace
        _current_trace.end()
        
        # ç¡®ä¿æ•°æ®è¢«å‘é€åˆ° Langfuse
        _langfuse_client.flush()
        print(f"âœ… æ•°æ®å·²åˆ·æ–°åˆ° Langfuse")
    
    # éªŒè¯ç»“æœ
    assert "messages" in result, "ç»“æœä¸­åº”åŒ…å« messages"
    assert "llm_response" in result, "ç»“æœä¸­åº”åŒ…å« llm_response"
    assert result["llm_response"], "LLM å“åº”ä¸åº”ä¸ºç©º"
    assert result["step_count"] == 3, f"åº”è¯¥æ‰§è¡Œäº†3æ­¥ï¼Œå®é™…æ‰§è¡Œäº†{result['step_count']}æ­¥"
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    print(f"\nğŸ“Š è¯·åœ¨ Langfuse Dashboard ä¸­æŸ¥çœ‹ç»“æœï¼š")
    print(f"   {langfuse_host}")
    print(f"   Trace ID: {_current_trace.trace_id}")
    print(f"\né¢„æœŸçœ‹åˆ°ï¼š")
    print(f"   - 1 ä¸ª Traceï¼ˆtest_02_llm_graphï¼‰")
    print(f"   - 3 ä¸ª Spanï¼ˆprepare_llm, call_llm, process_responseï¼‰")
    print(f"   - call_llm èŠ‚ç‚¹ä¸­åº”è¯¥æœ‰ Generation Spanï¼ˆLLM è°ƒç”¨ï¼‰")
    print(f"   - Generation Span åº”è¯¥æ˜¾ç¤ºï¼š")
    print(f"     * è¾“å…¥ prompt")
    print(f"     * è¾“å‡º response")
    print(f"     * Tokens ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœ LLM æ”¯æŒï¼‰")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    try:
        success = test_llm_graph_with_langfuse()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

