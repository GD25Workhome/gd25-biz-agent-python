# 思考过程调用方案

## 1. 背景与问题分析

### 1.1 当前状况

- **模型配置**：当前代码使用的模型为火山引擎的 r1 模型（DeepSeek R1），具备思考能力
- **问题**：虽然模型返回了思考过程（`reasoning_content`），但现有代码的日志系统和 Langfuse 都没有记录这些思考信息
- **影响**：无法观察和分析模型的推理过程，影响调试和优化

### 1.2 问题根源

根据测试和分析（参考 `cursor_test/LLM_Thinking/问题分析与解决方案.md`），问题的根源在于：

1. **火山引擎 API 原始响应包含思考过程**
   - API 响应格式：`response.choices[0].message.reasoning_content`
   - 该字段包含了完整的思考过程内容

2. **LangChain 响应处理丢失了思考过程**
   - LangChain 的 `ChatOpenAI` 在解析响应时，只提取了 `content` 字段
   - `reasoning_content` 字段没有被传递到 `AIMessage` 对象中
   - `response_metadata` 和 `llm_output` 中也没有 `reasoning_content`

3. **当前代码无法提取思考过程**
   - `llm_logger.py` 中已经实现了多种提取方式（结构化 content、标签提取、additional_kwargs 等）
   - 但由于 LangChain 根本没有保留 `reasoning_content`，这些方式都无法生效

### 1.3 火山引擎 API 响应格式

根据测试代码（`cursor_test/LLM_Thinking/test_final_solution.py`），火山引擎 API 的响应格式如下：

```json
{
  "choices": [
    {
      "message": {
        "content": "好的！已为您记录血压数据...",
        "reasoning_content": "嗯，用户让我帮忙记录血压数据，收缩压120，舒张压80。这组数字看起来是标准的理想血压值...",
        "role": "assistant"
      }
    }
  ],
  "usage": {
    "completion_tokens_details": {
      "reasoning_tokens": 240
    }
  }
}
```

**关键字段**：
- `choices[0].message.content`：最终答案
- `choices[0].message.reasoning_content`：思考过程（**需要提取**）
- `usage.completion_tokens_details.reasoning_tokens`：思考过程消耗的 token 数

## 2. 技术方案设计

### 2.1 方案选择

经过分析，推荐使用 **HTTP 响应拦截器方案**：

**优点**：
- 不需要修改 LangChain 源码
- 可以完整获取原始响应数据
- 对现有代码影响较小
- 实现相对简单

**缺点**：
- 需要维护 HTTP 客户端代码
- 需要处理响应体的读取和重建

### 2.2 架构设计

```
┌─────────────────┐
│   LangChain     │
│   ChatOpenAI    │
└────────┬────────┘
         │
         │ 使用自定义 HTTP 客户端
         ▼
┌─────────────────┐
│  HTTP 拦截器    │  ← 拦截响应，提取 reasoning_content
│ ResponseInterceptor│
└────────┬────────┘
         │
         │ 调用底层 Transport
         ▼
┌─────────────────┐
│  火山引擎 API   │
└─────────────────┘

提取的 reasoning_content 存储到全局字典：
_raw_api_responses[run_id] = {
    "reasoning_content": "...",
    "reasoning_tokens": 240,
    ...
}
```

### 2.3 数据流转

1. **请求阶段**：
   - `on_llm_start` 回调生成 `run_id`
   - 将 `run_id` 传递给 HTTP 拦截器（通过请求头或上下文）

2. **响应拦截阶段**：
   - HTTP 拦截器拦截响应
   - 解析 JSON 响应，提取 `reasoning_content`
   - 将 `reasoning_content` 存储到全局字典，使用 `run_id` 作为 key

3. **提取阶段**：
   - `on_llm_end` 回调中从全局字典提取 `reasoning_content`
   - 记录到日志和数据库
   - 传递给 Langfuse

## 3. 实现计划

### 3.1 阶段一：HTTP 响应拦截器实现

#### 3.1.1 创建响应拦截器模块

**文件**：`infrastructure/llm/response_interceptor.py`

**功能**：
- 拦截 HTTP 响应
- 解析 JSON 响应，提取 `reasoning_content`
- 将数据存储到全局字典

**关键实现点**：
1. 使用 `httpx.BaseTransport` 创建自定义 Transport
2. 在 `handle_request` 方法中拦截响应
3. 解析响应体，提取 `reasoning_content`
4. 重建响应对象（保持原始响应格式）

**技术难点**：
- 如何将 `run_id` 传递给拦截器？
  - 方案1：通过请求头传递（需要修改 LangChain 的请求构建）
  - 方案2：通过线程本地存储（thread-local storage）
  - 方案3：通过上下文变量（contextvars）

**推荐方案**：使用 `contextvars`，在 `on_llm_start` 中设置上下文变量，在拦截器中读取。

#### 3.1.2 修改 LLM 客户端

**文件**：`infrastructure/llm/client.py`

**修改内容**：
1. 导入响应拦截器
2. 创建自定义 HTTP 客户端（使用拦截器）
3. 将自定义客户端传递给 `ChatOpenAI`

**代码示例**：
```python
from infrastructure.llm.response_interceptor import create_http_client_with_interceptor

def get_llm(...):
    # ...
    http_client = create_http_client_with_interceptor()
    params["http_client"] = http_client
    # ...
```

### 3.2 阶段二：日志记录增强

#### 3.2.1 修改日志回调处理器

**文件**：`infrastructure/observability/llm_logger.py`

**修改内容**：
1. 在 `on_llm_start` 中设置上下文变量（存储 `run_id`）
2. 在 `on_llm_end` 中从全局字典提取 `reasoning_content`
3. 增强日志输出，包含思考过程
4. 数据库日志中保存思考过程

**日志格式**：
```
[LLM思考过程] call_id={call_id}
{reasoning_content}

[LLM响应内容] call_id={call_id}
{response_text}
```

**数据库字段**：
- `response_snapshot`：包含思考过程和最终答案的组合文本
- `response_messages`：新增 `role="reasoning"` 的消息记录

#### 3.2.2 Token 统计增强

**修改内容**：
- 提取 `reasoning_tokens`（思考过程消耗的 token）
- 在日志中显示 `reasoning_tokens` 信息
- 考虑在数据库中新增字段存储 `reasoning_tokens`

### 3.3 阶段三：Langfuse 对接

#### 3.3.1 了解 Langfuse 的思考过程支持

**调研内容**：
- Langfuse 是否支持记录思考过程？
- 如何将思考过程传递给 Langfuse？
- 思考过程在 Langfuse UI 中如何显示？

**可能的方式**：
1. 通过 `metadata` 传递思考过程
2. 通过 `output` 的扩展字段传递
3. 通过自定义字段传递

#### 3.3.2 修改 Langfuse Handler

**文件**：`infrastructure/observability/langfuse_handler.py`

**修改内容**：
1. 在 `CallbackHandler` 的回调中提取 `reasoning_content`
2. 将思考过程添加到 Langfuse 的追踪数据中

**实现方式**：
- 方案1：通过 `metadata` 传递
- 方案2：通过 `output` 的扩展字段传递
- 方案3：创建单独的 Span 记录思考过程

**推荐方案**：通过 `metadata` 传递，因为：
- 简单直接
- 不影响现有的 output 结构
- Langfuse 支持 metadata 的查询和过滤

### 3.4 阶段四：测试与验证

#### 3.4.1 单元测试

**测试文件**：`cursor_test/LLM_Thinking/test_reasoning_extraction_v2.py`

**测试内容**：
1. 测试 HTTP 拦截器能否正确提取 `reasoning_content`
2. 测试日志记录是否包含思考过程
3. 测试 Langfuse 是否记录了思考过程

#### 3.4.2 集成测试

**测试内容**：
1. 完整的 LLM 调用流程测试
2. 验证思考过程在日志中的显示
3. 验证思考过程在 Langfuse UI 中的显示

#### 3.4.3 性能测试

**测试内容**：
1. HTTP 拦截器对性能的影响
2. 思考过程记录对日志写入性能的影响
3. Langfuse 记录对整体性能的影响

## 4. 详细实现步骤

### 4.1 步骤1：创建响应拦截器（优先级：高）

**任务清单**：
- [ ] 创建 `infrastructure/llm/response_interceptor.py`
- [ ] 实现 `ResponseInterceptor` 类
- [ ] 实现 `create_http_client_with_interceptor` 函数
- [ ] 使用 `contextvars` 传递 `run_id`
- [ ] 实现响应拦截和 `reasoning_content` 提取
- [ ] 实现响应重建逻辑
- [ ] 编写单元测试

**预计工作量**：2-3 天

**技术难点**：
1. 响应体的读取和重建（需要处理流式响应）
2. `run_id` 的传递机制（使用 `contextvars`）

### 4.2 步骤2：集成响应拦截器到 LLM 客户端（优先级：高）

**任务清单**：
- [ ] 修改 `infrastructure/llm/client.py`
- [ ] 导入响应拦截器
- [ ] 创建自定义 HTTP 客户端
- [ ] 将客户端传递给 `ChatOpenAI`
- [ ] 测试 LLM 调用是否正常

**预计工作量**：0.5 天

### 4.3 步骤3：增强日志记录（优先级：高）

**任务清单**：
- [ ] 修改 `infrastructure/observability/llm_logger.py`
- [ ] 在 `on_llm_start` 中设置上下文变量
- [ ] 在 `on_llm_end` 中从全局字典提取 `reasoning_content`
- [ ] 增强控制台日志输出
- [ ] 增强数据库日志记录
- [ ] 提取和记录 `reasoning_tokens`
- [ ] 编写测试验证日志输出

**预计工作量**：1-2 天

### 4.4 步骤4：Langfuse 对接（优先级：中）

**任务清单**：
- [ ] 调研 Langfuse 的思考过程支持方式
- [ ] 修改 `infrastructure/observability/langfuse_handler.py`
- [ ] 在 Langfuse Callback 中提取 `reasoning_content`
- [ ] 将思考过程添加到 Langfuse 追踪数据
- [ ] 测试 Langfuse UI 中的显示

**预计工作量**：1-2 天

**技术难点**：
- Langfuse SDK 的 API 使用方式
- 思考过程在 Langfuse 中的最佳展示方式

### 4.5 步骤5：数据库字段扩展（可选，优先级：低）

**任务清单**：
- [ ] 设计数据库表结构（是否需要新增 `reasoning_tokens` 字段）
- [ ] 编写 Alembic 迁移脚本
- [ ] 修改 Repository 层代码
- [ ] 测试数据库写入

**预计工作量**：1 天

**注意**：如果 `reasoning_content` 已经保存在 `response_snapshot` 中，可能不需要新增字段。

### 4.6 步骤6：测试与文档（优先级：中）

**任务清单**：
- [ ] 编写单元测试
- [ ] 编写集成测试
- [ ] 编写性能测试
- [ ] 更新文档（使用说明、API 文档）
- [ ] 编写开发总结文档

**预计工作量**：2-3 天

## 5. 技术细节

### 5.1 上下文变量传递 run_id

**实现方式**：
```python
import contextvars

# 定义上下文变量
_run_id_context: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar('run_id', default=None)

# 在 on_llm_start 中设置
def on_llm_start(self, ...):
    run_id = str(kwargs.get("run_id") or uuid.uuid4())
    _run_id_context.set(run_id)
    # ...

# 在拦截器中读取
def handle_request(self, request):
    run_id = _run_id_context.get()
    # ...
```

### 5.2 响应拦截器实现要点

**关键代码结构**：
```python
class ResponseInterceptor(httpx.BaseTransport):
    def __init__(self, transport: httpx.BaseTransport):
        self._transport = transport
    
    def handle_request(self, request: httpx.Request) -> httpx.Response:
        # 调用底层 transport
        response = self._transport.handle_request(request)
        
        # 检查是否是 chat/completions 请求
        if "/chat/completions" in str(request.url):
            # 读取响应体
            response_body = response.read()
            
            # 解析 JSON
            try:
                response_data = json.loads(response_body.decode('utf-8'))
                
                # 提取 reasoning_content
                run_id = _run_id_context.get()
                if run_id and "choices" in response_data:
                    # 存储到全局字典
                    _raw_api_responses[run_id] = {
                        "reasoning_content": ...,
                        "reasoning_tokens": ...,
                    }
            except Exception:
                pass
            
            # 重建响应对象
            response = httpx.Response(
                status_code=response.status_code,
                headers=response.headers,
                content=response_body,
                request=request,
            )
        
        return response
```

### 5.3 日志格式设计

**控制台日志**：
```
[LLM请求开始] call_id={call_id}, model={model}, ...
[LLM请求提示词] call_id={call_id}
{prompt_snapshot}

[LLM响应完成] call_id={call_id}, latency={latency}ms, ...
[LLM思考过程] call_id={call_id}
{reasoning_content}
[LLM响应内容] call_id={call_id}
{response_text}
```

**数据库日志**：
- `response_snapshot`：`[思考过程]\n{reasoning_content}\n\n[最终答案]\n{response_text}`
- `response_messages`：包含 `role="reasoning"` 的消息

### 5.4 Langfuse 对接方式

**方案1：通过 metadata 传递**
```python
# 在 Langfuse Callback 中
metadata = {
    "reasoning_content": reasoning_content,
    "reasoning_tokens": reasoning_tokens,
}
# 添加到 generation 的 metadata
```

**方案2：通过 output 扩展**
```python
# 修改 output 结构
output = {
    "content": response_text,
    "reasoning": reasoning_content,
}
```

**推荐使用方案1**，因为更简单且不影响现有结构。

## 6. 风险评估与应对

### 6.1 技术风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| HTTP 拦截器实现复杂 | 高 | 参考 LangChain 的 HTTP 客户端实现，充分测试 |
| 响应体读取和重建失败 | 中 | 添加异常处理，失败时回退到原始响应 |
| contextvars 兼容性问题 | 低 | 使用标准库，Python 3.7+ 支持 |
| Langfuse API 不支持思考过程 | 中 | 通过 metadata 传递，不影响核心功能 |

### 6.2 性能风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| 响应拦截增加延迟 | 低 | 响应解析是同步操作，延迟可忽略 |
| 思考过程存储占用内存 | 低 | 使用全局字典，及时清理已使用的数据 |
| 日志写入增加 I/O | 中 | 使用异步写入，后台任务处理 |

### 6.3 兼容性风险

| 风险 | 影响 | 应对措施 |
|------|------|----------|
| LangChain 版本升级导致不兼容 | 中 | 锁定 LangChain 版本，升级时充分测试 |
| 火山引擎 API 格式变更 | 低 | 添加版本检测和兼容性处理 |

## 7. 验收标准

### 7.1 功能验收

- [ ] HTTP 拦截器能够正确提取 `reasoning_content`
- [ ] 控制台日志中显示思考过程
- [ ] 数据库日志中保存思考过程
- [ ] Langfuse UI 中能够查看思考过程
- [ ] 所有现有功能正常工作（回归测试通过）

### 7.2 性能验收

- [ ] HTTP 拦截器增加的延迟 < 10ms
- [ ] 日志写入不影响主流程性能
- [ ] Langfuse 记录不影响主流程性能

### 7.3 质量验收

- [ ] 单元测试覆盖率 > 80%
- [ ] 集成测试通过
- [ ] 代码审查通过
- [ ] 文档完整

## 8. 后续优化方向

### 8.1 短期优化

1. **思考过程格式化**：对思考过程进行格式化，提高可读性
2. **思考过程分析**：分析思考过程的长度、复杂度等指标
3. **思考过程搜索**：在日志系统中支持思考过程的搜索和过滤

### 8.2 长期优化

1. **思考过程可视化**：在 Langfuse UI 中可视化思考过程
2. **思考过程优化**：基于思考过程分析，优化提示词和模型参数
3. **思考过程缓存**：对于相似的输入，缓存思考过程（如果模型支持）

## 9. 参考资料

- [问题分析与解决方案](./cursor_test/LLM_Thinking/问题分析与解决方案.md)
- [测试代码](./cursor_test/LLM_Thinking/test_final_solution.py)
- [LangChain ChatOpenAI 文档](https://python.langchain.com/docs/integrations/chat/openai)
- [火山引擎 DeepSeek R1 模型文档](https://www.volcengine.com/docs/82379/1554373)
- [Langfuse 文档](https://langfuse.com/docs)

## 10. 附录

### 10.1 相关文件清单

**需要修改的文件**：
- `infrastructure/llm/client.py` - 集成响应拦截器
- `infrastructure/observability/llm_logger.py` - 增强日志记录
- `infrastructure/observability/langfuse_handler.py` - Langfuse 对接

**需要创建的文件**：
- `infrastructure/llm/response_interceptor.py` - HTTP 响应拦截器

**测试文件**：
- `cursor_test/LLM_Thinking/test_reasoning_extraction_v2.py` - 新的测试文件

### 10.2 关键代码位置

- 日志回调处理器：`infrastructure/observability/llm_logger.py:194-527`
- Langfuse Handler：`infrastructure/observability/langfuse_handler.py:69-119`
- LLM 客户端：`infrastructure/llm/client.py:19-106`

