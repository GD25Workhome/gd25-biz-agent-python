# 审核、安全、回复内容评估集成方案

## 一、方案概述

在 LangGraphFlow 多智能体路由系统中，可以通过在路由图中添加新的节点来实现审核、安全和回复内容评估功能。这些功能可以作为独立的节点插入到执行流程中，确保系统的安全性和内容质量。

## 二、架构设计

### 2.1 修改后的整体架构

```
┌─────────────────────────────────────────────────────────┐
│                   用户请求入口                            │
│              (FastAPI Backend Server)                    │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│           输入安全检查节点 (Input Security Check)        │
│  - 输入内容安全检查                                       │
│  - 敏感信息过滤                                          │
│  - 权限验证                                              │
│  - 频率限制检查                                          │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│           输入审核节点 (Input Review)                    │
│  - 内容合规性检查                                        │
│  - 恶意内容检测                                          │
│  - 用户行为分析                                          │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│              路由智能体 (Router Agent)                   │
│  基于StateGraph实现，每次调用都经过路由节点               │
│  - 意图识别                                             │
│  - 意图澄清                                             │
│  - 路由决策                                             │
│  - 自动重新路由                                         │
└─────┬───────────┬───────────┬───────────┬──────────────┘
      │           │           │           │
      ▼           ▼           ▼           ▼
┌──────────┐ ┌──────────┐ ┌──────────────────┐ ┌──────────┐
│ 血压记录 │ │ 复诊管理 │ │  诊断智能体系统   │ │ 其他智能体│
│ 智能体   │ │ 智能体   │ │  ├─ 内科诊断      │ │ (可扩展) │
│          │ │          │ │  ├─ 外科诊断      │ │          │
│          │ │          │ │  ├─ 儿科诊断      │ │          │
│          │ │          │ │  ├─ 妇科诊断      │ │          │
│          │ │          │ │  ├─ 心血管科诊断  │ │          │
│          │ │          │ │  └─ 通用诊断      │ │          │
└─────┬────┘ └─────┬────┘ └─────┬──────────────┘ └─────┬────┘
      │           │           │           │
      └───────────┴───────────┴───────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│         回复内容评估节点 (Response Evaluation)           │
│  - 回复内容质量评估                                      │
│  - 安全性检查                                            │
│  - 准确性验证                                            │
│  - 合规性检查                                            │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│           回复审核节点 (Response Review)                 │
│  - 内容审核                                              │
│  - 敏感信息检测                                          │
│  - 医疗建议风险评估                                      │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
          ┌──────────────────────┐
          │   返回到路由节点      │
          │  (通过StateGraph边)  │
          └──────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│              基础设施层                                   │
│  - PostgreSQL (短期记忆 + 长期记忆 + 业务数据)           │
│  - Redis (会话管理 + 安全策略缓存)                       │
│  - 向量数据库 (RAG知识库: PgVector 或 ADB PG)          │
│  - LangGraph (Agent框架)                                │
│  - Java微服务 (业务功能)                                │
│  - 审核服务 (可选，外部审核API)                         │
└─────────────────────────────────────────────────────────┘
```

### 2.2 执行流程

```
用户请求
    ↓
输入安全检查节点
    ├─ 检查通过 → 继续
    └─ 检查失败 → 返回错误消息，停止执行
    ↓
输入审核节点
    ├─ 审核通过 → 继续
    └─ 审核失败 → 返回审核失败消息，停止执行
    ↓
路由智能体节点
    ↓
专门智能体节点（血压记录/复诊管理/诊断等）
    ↓
回复内容评估节点
    ├─ 评估通过 → 继续
    └─ 评估失败 → 标记为需要人工审核或返回安全回复
    ↓
回复审核节点
    ├─ 审核通过 → 返回给用户
    └─ 审核失败 → 返回安全回复或标记为需要人工审核
    ↓
返回到路由节点（等待下次调用）
```

## 三、详细设计

### 3.1 状态扩展

需要在 `RouterState` 中添加安全相关的状态字段：

```python
class RouterState(TypedDict):
    """路由状态数据结构"""
    messages: List[BaseMessage]  # 消息列表
    current_intent: Optional[str]  # 当前意图
    sub_intent: Optional[str]  # 子意图
    current_agent: Optional[str]  # 当前活跃的智能体名称
    need_reroute: bool  # 是否需要重新路由
    session_id: str  # 会话ID
    user_id: str  # 用户ID
    
    # 新增：安全相关状态
    security_check_passed: bool  # 输入安全检查是否通过
    input_review_passed: bool  # 输入审核是否通过
    response_evaluation_result: Optional[Dict[str, Any]]  # 回复内容评估结果
    response_review_passed: bool  # 回复审核是否通过
    security_risk_level: Optional[str]  # 安全风险等级：low/medium/high
    review_required: bool  # 是否需要人工审核
```

### 3.2 输入安全检查节点

**功能**：在用户输入进入系统前进行安全检查

**检查项**：
1. **输入内容安全检查**
   - SQL注入检测
   - XSS攻击检测
   - 命令注入检测
   - 路径遍历检测

2. **敏感信息过滤**
   - 检测是否包含敏感词（政治、色情、暴力等）
   - 检测是否包含个人信息泄露风险

3. **权限验证**
   - 验证用户是否有权限访问该功能
   - 验证用户角色是否匹配

4. **频率限制检查**
   - 检查用户请求频率是否超过限制
   - 防止恶意刷屏和DDoS攻击

**实现代码示例**：

```python
"""
输入安全检查节点
"""
import logging
import re
from typing import Dict, Any
from langchain_core.messages import AIMessage, HumanMessage
from domain.router import RouterState
from core.config import get_settings

logger = logging.getLogger(__name__)


def input_security_check_node(state: RouterState) -> RouterState:
    """
    输入安全检查节点
    
    Args:
        state: 路由状态
        
    Returns:
        RouterState: 更新后的路由状态
    """
    messages = state.get("messages", [])
    if not messages:
        return state
    
    last_message = messages[-1]
    
    # 只检查用户消息
    if not isinstance(last_message, HumanMessage):
        # 如果不是用户消息，直接通过
        updated_state = state.copy()
        updated_state["security_check_passed"] = True
        return updated_state
    
    user_query = last_message.content
    user_id = state.get("user_id", "")
    
    # 1. SQL注入检测
    sql_injection_patterns = [
        r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER|EXEC|UNION)\b)",
        r"(--|#|/\*|\*/)",
        r"(\bor\b\s+\d+\s*=\s*\d+)",
    ]
    for pattern in sql_injection_patterns:
        if re.search(pattern, user_query, re.IGNORECASE):
            logger.warning(f"检测到SQL注入尝试: user_id={user_id}, query={user_query[:50]}")
            updated_state = state.copy()
            updated_state["security_check_passed"] = False
            updated_state["security_risk_level"] = "high"
            # 添加错误消息
            error_msg = AIMessage(content="检测到不安全的内容，请求已被拒绝。")
            updated_state["messages"] = list(messages) + [error_msg]
            return updated_state
    
    # 2. XSS攻击检测
    xss_patterns = [
        r"<script[^>]*>.*?</script>",
        r"javascript:",
        r"on\w+\s*=",
    ]
    for pattern in xss_patterns:
        if re.search(pattern, user_query, re.IGNORECASE):
            logger.warning(f"检测到XSS攻击尝试: user_id={user_id}, query={user_query[:50]}")
            updated_state = state.copy()
            updated_state["security_check_passed"] = False
            updated_state["security_risk_level"] = "high"
            error_msg = AIMessage(content="检测到不安全的内容，请求已被拒绝。")
            updated_state["messages"] = list(messages) + [error_msg]
            return updated_state
    
    # 3. 敏感词检测（示例，实际应从配置文件或数据库加载）
    sensitive_words = ["敏感词1", "敏感词2"]  # 从配置加载
    for word in sensitive_words:
        if word in user_query:
            logger.warning(f"检测到敏感词: user_id={user_id}, word={word}")
            updated_state = state.copy()
            updated_state["security_check_passed"] = False
            updated_state["security_risk_level"] = "medium"
            error_msg = AIMessage(content="您的内容包含不当信息，请重新输入。")
            updated_state["messages"] = list(messages) + [error_msg]
            return updated_state
    
    # 4. 频率限制检查（使用Redis）
    # 这里需要实现频率限制逻辑
    # if await check_rate_limit(user_id):
    #     updated_state = state.copy()
    #     updated_state["security_check_passed"] = False
    #     error_msg = AIMessage(content="请求过于频繁，请稍后再试。")
    #     updated_state["messages"] = list(messages) + [error_msg]
    #     return updated_state
    
    # 所有检查通过
    updated_state = state.copy()
    updated_state["security_check_passed"] = True
    updated_state["security_risk_level"] = "low"
    logger.debug(f"输入安全检查通过: user_id={user_id}")
    
    return updated_state
```

### 3.3 输入审核节点

**功能**：对用户输入进行内容审核

**检查项**：
1. **内容合规性检查**
   - 检查内容是否符合法律法规
   - 检查内容是否符合医疗行业规范

2. **恶意内容检测**
   - 使用LLM或规则引擎检测恶意内容
   - 检测是否包含攻击性、歧视性内容

3. **用户行为分析**
   - 分析用户历史行为
   - 检测异常行为模式

**实现代码示例**：

```python
"""
输入审核节点
"""
import logging
from langchain_core.messages import AIMessage, HumanMessage
from domain.router import RouterState
from core.llm import get_llm_by_config
from langchain_core.prompts import ChatPromptTemplate

logger = logging.getLogger(__name__)


def input_review_node(state: RouterState) -> RouterState:
    """
    输入审核节点
    
    Args:
        state: 路由状态
        
    Returns:
        RouterState: 更新后的路由状态
    """
    # 如果安全检查未通过，跳过审核
    if not state.get("security_check_passed", True):
        updated_state = state.copy()
        updated_state["input_review_passed"] = False
        return updated_state
    
    messages = state.get("messages", [])
    if not messages:
        updated_state = state.copy()
        updated_state["input_review_passed"] = True
        return updated_state
    
    last_message = messages[-1]
    
    # 只审核用户消息
    if not isinstance(last_message, HumanMessage):
        updated_state = state.copy()
        updated_state["input_review_passed"] = True
        return updated_state
    
    user_query = last_message.content
    user_id = state.get("user_id", "")
    
    try:
        # 使用LLM进行内容审核
        review_prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个内容审核专家，负责审核用户输入的内容是否符合医疗健康场景的要求。

审核标准：
1. 内容是否合法合规
2. 内容是否包含恶意、攻击性、歧视性内容
3. 内容是否符合医疗健康场景的使用规范
4. 内容是否包含不当的医疗建议请求

请返回JSON格式的审核结果：
{
    "passed": true/false,
    "risk_level": "low/medium/high",
    "reason": "审核理由",
    "suggestion": "建议（如果未通过）"
}"""),
            ("human", "用户输入内容：{user_query}\n\n请进行内容审核。")
        ])
        
        llm = get_llm_by_config()
        chain = review_prompt | llm
        response = chain.invoke({"user_query": user_query})
        
        # 解析审核结果
        import json
        review_text = response.content if hasattr(response, 'content') else str(response)
        review_result = json.loads(review_text)
        
        passed = review_result.get("passed", True)
        risk_level = review_result.get("risk_level", "low")
        
        if not passed:
            logger.warning(f"输入审核未通过: user_id={user_id}, reason={review_result.get('reason')}")
            updated_state = state.copy()
            updated_state["input_review_passed"] = False
            updated_state["security_risk_level"] = risk_level
            error_msg = AIMessage(content=review_result.get("suggestion", "您的内容未通过审核，请重新输入。"))
            updated_state["messages"] = list(messages) + [error_msg]
            return updated_state
        
        # 审核通过
        updated_state = state.copy()
        updated_state["input_review_passed"] = True
        logger.debug(f"输入审核通过: user_id={user_id}")
        
        return updated_state
        
    except Exception as e:
        logger.error(f"输入审核节点执行失败: {str(e)}")
        # 审核失败时，默认通过（避免阻塞正常流程）
        updated_state = state.copy()
        updated_state["input_review_passed"] = True
        return updated_state
```

### 3.4 回复内容评估节点

**功能**：对智能体生成的回复进行质量评估

**评估项**：
1. **回复内容质量评估**
   - 相关性评估
   - 完整性评估
   - 准确性评估

2. **安全性检查**
   - 检查回复是否包含敏感信息
   - 检查回复是否包含不当建议

3. **准确性验证**
   - 验证医疗建议的准确性
   - 验证数据的正确性

4. **合规性检查**
   - 检查是否符合医疗行业规范
   - 检查是否符合法律法规

**实现代码示例**：

```python
"""
回复内容评估节点
"""
import logging
from typing import Dict, Any
from langchain_core.messages import AIMessage
from domain.router import RouterState
from core.llm import get_llm_by_config
from langchain_core.prompts import ChatPromptTemplate

logger = logging.getLogger(__name__)


def response_evaluation_node(state: RouterState) -> RouterState:
    """
    回复内容评估节点
    
    Args:
        state: 路由状态
        
    Returns:
        RouterState: 更新后的路由状态
    """
    messages = state.get("messages", [])
    if not messages:
        updated_state = state.copy()
        updated_state["response_evaluation_result"] = None
        return updated_state
    
    # 获取最后一条AI消息
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break
    
    if not last_ai_message:
        updated_state = state.copy()
        updated_state["response_evaluation_result"] = None
        return updated_state
    
    response_content = last_ai_message.content
    user_id = state.get("user_id", "")
    current_agent = state.get("current_agent", "")
    
    try:
        # 使用LLM进行回复内容评估
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个内容评估专家，负责评估AI助手生成的回复内容。

评估标准：
1. **相关性**：回复是否与用户问题相关（0-1分）
2. **完整性**：回复是否完整回答了用户问题（0-1分）
3. **准确性**：回复内容是否准确，特别是医疗建议（0-1分）
4. **安全性**：回复是否包含敏感信息或不当建议（0-1分，越高越安全）
5. **合规性**：回复是否符合医疗行业规范和法律法规（0-1分）

请返回JSON格式的评估结果：
{
    "relevance_score": 0.0-1.0,
    "completeness_score": 0.0-1.0,
    "accuracy_score": 0.0-1.0,
    "safety_score": 0.0-1.0,
    "compliance_score": 0.0-1.0,
    "overall_score": 0.0-1.0,
    "passed": true/false,
    "risk_level": "low/medium/high",
    "issues": ["问题1", "问题2"],
    "suggestions": ["建议1", "建议2"]
}"""),
            ("human", """AI助手回复内容：{response_content}

当前智能体：{current_agent}

请对回复内容进行全面评估。""")
        ])
        
        llm = get_llm_by_config()
        chain = evaluation_prompt | llm
        response = chain.invoke({
            "response_content": response_content,
            "current_agent": current_agent
        })
        
        # 解析评估结果
        import json
        eval_text = response.content if hasattr(response, 'content') else str(response)
        eval_result = json.loads(eval_text)
        
        overall_score = eval_result.get("overall_score", 0.0)
        passed = eval_result.get("passed", overall_score >= 0.7)
        risk_level = eval_result.get("risk_level", "low")
        
        # 保存评估结果
        updated_state = state.copy()
        updated_state["response_evaluation_result"] = eval_result
        
        if not passed:
            logger.warning(f"回复内容评估未通过: user_id={user_id}, score={overall_score}, issues={eval_result.get('issues')}")
            updated_state["review_required"] = True
            updated_state["security_risk_level"] = risk_level
        
        logger.info(f"回复内容评估完成: user_id={user_id}, overall_score={overall_score}, passed={passed}")
        
        return updated_state
        
    except Exception as e:
        logger.error(f"回复内容评估节点执行失败: {str(e)}")
        # 评估失败时，默认通过
        updated_state = state.copy()
        updated_state["response_evaluation_result"] = None
        return updated_state
```

### 3.5 回复审核节点

**功能**：对智能体生成的回复进行最终审核

**检查项**：
1. **内容审核**
   - 检查回复是否包含不当内容
   - 检查回复是否符合医疗规范

2. **敏感信息检测**
   - 检测是否泄露用户隐私
   - 检测是否包含敏感医疗信息

3. **医疗建议风险评估**
   - 评估医疗建议的风险等级
   - 对于高风险建议，标记为需要人工审核

**实现代码示例**：

```python
"""
回复审核节点
"""
import logging
from langchain_core.messages import AIMessage
from domain.router import RouterState
from core.llm import get_llm_by_config
from langchain_core.prompts import ChatPromptTemplate

logger = logging.getLogger(__name__)


def response_review_node(state: RouterState) -> RouterState:
    """
    回复审核节点
    
    Args:
        state: 路由状态
        
    Returns:
        RouterState: 更新后的路由状态
    """
    # 如果评估未通过，需要更严格的审核
    eval_result = state.get("response_evaluation_result")
    if eval_result and not eval_result.get("passed", True):
        # 评估未通过，标记为需要人工审核
        updated_state = state.copy()
        updated_state["response_review_passed"] = False
        updated_state["review_required"] = True
        
        # 可以选择返回安全回复
        messages = state.get("messages", [])
        safe_response = AIMessage(
            content="抱歉，系统检测到回复内容可能需要进一步审核。为了您的安全，请咨询专业医生获取更准确的建议。"
        )
        updated_state["messages"] = list(messages) + [safe_response]
        
        logger.warning(f"回复审核未通过，需要人工审核: user_id={state.get('user_id')}")
        return updated_state
    
    messages = state.get("messages", [])
    if not messages:
        updated_state = state.copy()
        updated_state["response_review_passed"] = True
        return updated_state
    
    # 获取最后一条AI消息
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break
    
    if not last_ai_message:
        updated_state = state.copy()
        updated_state["response_review_passed"] = True
        return updated_state
    
    response_content = last_ai_message.content
    user_id = state.get("user_id", "")
    
    try:
        # 使用LLM进行回复审核
        review_prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一个医疗内容审核专家，负责审核AI助手生成的医疗相关回复。

审核标准：
1. **内容合规性**：回复是否符合医疗行业规范和法律法规
2. **安全性**：回复是否包含不当的医疗建议或风险提示
3. **准确性**：回复中的医疗信息是否准确
4. **风险等级**：评估回复中医疗建议的风险等级

请返回JSON格式的审核结果：
{
    "passed": true/false,
    "risk_level": "low/medium/high",
    "medical_advice_risk": "low/medium/high",
    "issues": ["问题1", "问题2"],
    "requires_human_review": true/false,
    "suggestion": "建议（如果需要修改）"
}"""),
            ("human", "AI助手回复内容：{response_content}\n\n请进行医疗内容审核。")
        ])
        
        llm = get_llm_by_config()
        chain = review_prompt | llm
        response = chain.invoke({"response_content": response_content})
        
        # 解析审核结果
        import json
        review_text = response.content if hasattr(response, 'content') else str(response)
        review_result = json.loads(review_text)
        
        passed = review_result.get("passed", True)
        risk_level = review_result.get("risk_level", "low")
        requires_human_review = review_result.get("requires_human_review", False)
        medical_advice_risk = review_result.get("medical_advice_risk", "low")
        
        updated_state = state.copy()
        updated_state["response_review_passed"] = passed
        updated_state["security_risk_level"] = risk_level
        updated_state["review_required"] = requires_human_review
        
        if not passed or requires_human_review:
            logger.warning(f"回复审核未通过或需要人工审核: user_id={user_id}, risk_level={risk_level}, medical_advice_risk={medical_advice_risk}")
            
            # 对于高风险医疗建议，返回安全回复
            if medical_advice_risk in ["high"]:
                safe_response = AIMessage(
                    content="抱歉，系统检测到您的咨询涉及高风险医疗建议。为了您的安全，建议您咨询专业医生获取更准确的诊断和治疗建议。"
                )
                updated_state["messages"] = list(messages) + [safe_response]
        
        logger.info(f"回复审核完成: user_id={user_id}, passed={passed}, requires_human_review={requires_human_review}")
        
        return updated_state
        
    except Exception as e:
        logger.error(f"回复审核节点执行失败: {str(e)}")
        # 审核失败时，默认通过（避免阻塞正常流程）
        updated_state = state.copy()
        updated_state["response_review_passed"] = True
        return updated_state
```

## 四、路由图修改

### 4.1 修改路由图构建函数

需要在 `create_router_graph` 函数中添加新的节点和边：

```python
def create_router_graph(checkpointer: AsyncPostgresSaver, pool: AsyncConnectionPool, store: AsyncPostgresStore = None):
    """
    创建路由图（包含安全审核节点）
    """
    from domain.security import (
        input_security_check_node,
        input_review_node,
        response_evaluation_node,
        response_review_node
    )
    
    # 创建StateGraph
    router_graph = StateGraph(RouterState)
    
    # 添加安全审核节点
    router_graph.add_node("input_security_check", input_security_check_node)
    router_graph.add_node("input_review", input_review_node)
    router_graph.add_node("router", router_node)
    router_graph.add_node("clarify_intent", clarify_intent_node)
    
    # 添加专门智能体节点（原有节点）
    # ... 原有代码 ...
    
    # 添加回复评估和审核节点
    router_graph.add_node("response_evaluation", response_evaluation_node)
    router_graph.add_node("response_review", response_review_node)
    
    # 设置入口点：从输入安全检查开始
    router_graph.set_entry_point("input_security_check")
    
    # 添加边：输入安全检查 -> 输入审核 -> 路由节点
    router_graph.add_edge("input_security_check", "input_review")
    router_graph.add_edge("input_review", "router")
    
    # 添加条件边：路由节点 -> 专门智能体（原有逻辑）
    router_graph.add_conditional_edges(
        "router",
        route_decision,
        {
            "blood_pressure": "blood_pressure_agent",
            "appointment": "appointment_agent",
            # ... 其他路由 ...
            "unclear": "clarify_intent",
            "__end__": "__end__"
        }
    )
    
    # 修改回边：专门智能体 -> 回复内容评估 -> 回复审核 -> 路由节点
    # 注意：所有专门智能体执行完后，都先经过回复评估和审核
    router_graph.add_edge("blood_pressure_agent", "response_evaluation")
    router_graph.add_edge("appointment_agent", "response_evaluation")
    # ... 其他智能体 ...
    router_graph.add_edge("clarify_intent", "response_evaluation")
    
    router_graph.add_edge("response_evaluation", "response_review")
    router_graph.add_edge("response_review", "router")
    
    # 编译图
    compiled_graph = router_graph.compile(checkpointer=checkpointer)
    
    logger.info("路由图创建成功（包含安全审核节点）")
    
    return compiled_graph
```

### 4.2 路由决策函数修改

需要在路由决策函数中考虑安全检查结果：

```python
def route_decision(state: RouterState) -> Literal[...]:
    """
    路由决策函数（考虑安全检查结果）
    """
    # 如果安全检查未通过，停止执行
    if not state.get("security_check_passed", True):
        return "__end__"
    
    # 如果输入审核未通过，停止执行
    if not state.get("input_review_passed", True):
        return "__end__"
    
    # 原有路由逻辑
    messages = state.get("messages", [])
    if messages:
        last_message = messages[-1]
        if isinstance(last_message, AIMessage):
            return "__end__"
    
    current_intent = state.get("current_intent", "unclear")
    
    if current_intent == "unclear":
        return "unclear"
    
    return current_intent
```

## 五、配置和开关

### 5.1 配置项

在配置文件中添加安全审核相关的配置：

```python
# core/config/settings.py

class Settings(BaseSettings):
    # ... 原有配置 ...
    
    # 安全审核配置
    enable_input_security_check: bool = True  # 是否启用输入安全检查
    enable_input_review: bool = True  # 是否启用输入审核
    enable_response_evaluation: bool = True  # 是否启用回复内容评估
    enable_response_review: bool = True  # 是否启用回复审核
    
    # 安全检查配置
    security_check_sql_injection: bool = True  # SQL注入检测
    security_check_xss: bool = True  # XSS攻击检测
    security_check_sensitive_words: bool = True  # 敏感词检测
    security_rate_limit_enabled: bool = True  # 频率限制
    security_rate_limit_per_minute: int = 60  # 每分钟请求限制
    
    # 审核配置
    review_llm_model: str = "deepseek-chat"  # 审核使用的LLM模型
    review_threshold: float = 0.7  # 审核通过阈值
    review_require_human_for_high_risk: bool = True  # 高风险内容是否需要人工审核
    
    # 评估配置
    evaluation_min_score: float = 0.7  # 评估最低分数
    evaluation_auto_reject_below_score: bool = False  # 是否自动拒绝低于最低分数的回复
```

### 5.2 开关控制

在节点函数中根据配置决定是否执行：

```python
def input_security_check_node(state: RouterState) -> RouterState:
    """输入安全检查节点"""
    settings = get_settings()
    
    # 如果未启用，直接通过
    if not settings.enable_input_security_check:
        updated_state = state.copy()
        updated_state["security_check_passed"] = True
        return updated_state
    
    # 执行安全检查逻辑
    # ... 原有代码 ...
```

## 六、实施步骤

### 6.1 第一阶段：基础安全检查

1. 实现输入安全检查节点
   - SQL注入检测
   - XSS攻击检测
   - 敏感词检测

2. 修改路由图
   - 添加输入安全检查节点
   - 修改入口点

3. 测试验证
   - 测试各种攻击场景
   - 验证安全检查是否有效

### 6.2 第二阶段：内容审核

1. 实现输入审核节点
   - 使用LLM进行内容审核
   - 实现审核结果解析

2. 实现回复审核节点
   - 医疗内容审核
   - 风险评估

3. 测试验证
   - 测试各种内容场景
   - 验证审核准确性

### 6.3 第三阶段：回复内容评估

1. 实现回复内容评估节点
   - 多维度评估
   - 评分机制

2. 集成到路由图
   - 添加评估节点
   - 修改回边逻辑

3. 测试验证
   - 测试评估准确性
   - 优化评估标准

### 6.4 第四阶段：优化和监控

1. 性能优化
   - 缓存审核结果
   - 异步处理

2. 监控和日志
   - 记录审核结果
   - 统计分析

3. 人工审核集成
   - 高风险内容标记
   - 人工审核流程

## 七、注意事项

### 7.1 性能考虑

1. **LLM调用开销**：审核和评估节点会调用LLM，增加响应时间
   - 解决方案：使用缓存、异步处理、批量处理

2. **并发处理**：多个安全检查可能影响并发性能
   - 解决方案：使用异步IO、连接池、限流

### 7.2 准确性考虑

1. **误判问题**：安全检查可能误判正常内容
   - 解决方案：优化检测规则、使用白名单、人工复核

2. **审核准确性**：LLM审核可能不准确
   - 解决方案：使用多个模型、人工审核、持续优化

### 7.3 用户体验考虑

1. **响应时间**：增加审核节点会增加响应时间
   - 解决方案：异步处理、缓存、优化审核逻辑

2. **错误提示**：审核失败时的错误提示要友好
   - 解决方案：提供清晰的错误信息、建议用户修改

## 八、总结

通过在 LangGraph StateGraph 架构中添加安全审核节点，可以实现：

1. **输入安全检查**：在用户输入进入系统前进行安全检查
2. **输入审核**：对用户输入进行内容审核
3. **回复内容评估**：对智能体生成的回复进行质量评估
4. **回复审核**：对智能体生成的回复进行最终审核

这种设计具有以下优点：

- ✅ **架构清晰**：安全审核作为独立节点，不影响现有功能
- ✅ **易于控制**：可以通过配置开关控制是否启用
- ✅ **易于扩展**：可以轻松添加新的安全检查项
- ✅ **易于测试**：每个节点可以独立测试

---

**文档版本**：V1.0  
**创建时间**：2025-01-XX  
**维护者**：开发团队

