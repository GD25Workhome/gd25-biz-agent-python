"""
LLM å®¢æˆ·ç«¯æµ‹è¯•
æµ‹è¯•ç«å±±å¼•æ“æ¨¡å‹è¿é€šæ€§

Pytest å‘½ä»¤ç¤ºä¾‹ï¼š
================

# è¿è¡Œæ•´ä¸ªæµ‹è¯•æ–‡ä»¶
pytest cursor_test/M1_test/infrastructure/test_llm_client.py

# è¿è¡Œæ•´ä¸ªæµ‹è¯•æ–‡ä»¶ï¼ˆè¯¦ç»†è¾“å‡ºï¼‰
pytest cursor_test/M1_test/infrastructure/test_llm_client.py -v
# ï¼ˆè¯¦ç»†è¾“å‡º + æ˜¾ç¤º print è¾“å‡ºï¼‰
pytest cursor_test/M1_test/infrastructure/test_llm_client.py -v -s

# è¿è¡Œç‰¹å®šçš„æµ‹è¯•æ–¹æ³•
pytest cursor_test/M1_test/infrastructure/test_llm_client.py::TestLLMClient::test_volcengine_connection
"""
import pytest
from langchain_core.messages import HumanMessage

from infrastructure.llm.client import get_llm
from app.core.config import settings


class TestLLMClient:
    """LLM å®¢æˆ·ç«¯æµ‹è¯•ç±»"""
    
    @pytest.mark.asyncio
    async def test_volcengine_connection(self):
        """
        æµ‹è¯•ç”¨ä¾‹ï¼šéªŒè¯ç«å±±å¼•æ“ API è¿é€šæ€§
        
        éªŒè¯ï¼š
        - èƒ½å¤ŸæˆåŠŸåˆ›å»º LLM å®¢æˆ·ç«¯å®ä¾‹
        - èƒ½å¤Ÿæ­£å¸¸è¿æ¥åˆ°ç«å±±å¼•æ“ API
        - èƒ½å¤ŸæˆåŠŸè°ƒç”¨ API å¹¶è·å–å“åº”
        """
        # Arrangeï¼ˆå‡†å¤‡ï¼‰
        # ä½¿ç”¨é…ç½®ä¸­çš„ç«å±±å¼•æ“é…ç½®
        # é…ç½®æ¥æºï¼š.env æ–‡ä»¶
        # OPENAI_API_KEY=48bafa4f-***-ca2e2f755522
        # OPENAI_BASE_URL=https://ark.cn-beijing.volces.com/api/v3/chat/completions
        # LLM_MODEL=doubao-seed-1-6-251015
        # LLM_TEMPERATURE=0.7
        
        # Actï¼ˆæ‰§è¡Œï¼‰
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼åˆ›å»º LLM å®ä¾‹
        llm = get_llm()
        
        # éªŒè¯ LLM å®ä¾‹åˆ›å»ºæˆåŠŸ
        assert llm is not None
        
        # å‘é€æµ‹è¯•æ¶ˆæ¯
        messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'")]
        response = await llm.ainvoke(messages)
        
        # Assertï¼ˆæ–­è¨€ï¼‰
        # éªŒè¯å“åº”ä¸ä¸ºç©º
        assert response is not None
        assert hasattr(response, 'content')
        assert response.content is not None
        assert len(response.content) > 0
        
        # æ‰“å°å“åº”å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"\nâœ… ç«å±±å¼•æ“ API è¿é€šæ€§æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“ æ¨¡å‹: {settings.LLM_MODEL}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {settings.LLM_TEMPERATURE}")
        print(f"ğŸ”— Base URL: {settings.OPENAI_BASE_URL}")
        print(f"ğŸ’¬ å“åº”å†…å®¹: {response.content[:200]}...")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
