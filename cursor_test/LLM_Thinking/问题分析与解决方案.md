# DeepSeek R1 思考过程缺失问题分析与解决方案

## 问题描述

在使用火山引擎的 DeepSeek R1 模型（`deepseek-r1-250528`）时，LLM 响应日志中没有显示模型的思考过程（reasoning process）。

## 问题根源

通过测试发现：

1. **原始 API 响应包含思考过程**
   - 火山引擎 API 的原始响应中，`message` 对象包含 `reasoning_content` 字段
   - 该字段包含了完整的思考过程内容

2. **LangChain 响应处理丢失了思考过程**
   - LangChain 的 `ChatOpenAI` 在解析响应时，只提取了 `content` 字段
   - `reasoning_content` 字段没有被传递到 `AIMessage` 对象中
   - `response_metadata` 和 `llm_output` 中也没有 `reasoning_content`

3. **当前代码无法提取思考过程**
   - `llm_logger.py` 中的提取逻辑尝试了多种方式：
     - 从结构化 content 列表中提取
     - 从 `<think>` 标签中提取
     - 从 `additional_kwargs` 中查找
   - 但这些方式都无法获取到 `reasoning_content`，因为它根本不在 LangChain 的响应对象中

## 测试结果

### 测试1: 直接 API 调用

```json
{
  "choices": [
    {
      "message": {
        "content": "好的！已为您记录血压数据...",
        "reasoning_content": "嗯，用户让我帮忙记录血压数据，收缩压120，舒张压80。这组数字看起来是标准的理想血压值...",
        "role": "assistant"
      }
    }
  ],
  "usage": {
    "completion_tokens_details": {
      "reasoning_tokens": 240
    }
  }
}
```

✅ **原始 API 响应中包含 `reasoning_content` 字段**

### 测试2: LangChain 响应对象

```python
response.content  # 只包含 content 字段
response.additional_kwargs  # {'refusal': None}
response.response_metadata  # 只有 token_usage，没有 reasoning_content
```

❌ **LangChain 响应对象中没有 `reasoning_content`**

## 解决方案

### 方案1: 使用自定义 HTTP 客户端拦截响应（推荐）

创建一个自定义的 HTTP 客户端，在响应解析前拦截原始响应，提取 `reasoning_content` 并存储。

**优点**：
- 不需要修改 LangChain 源码
- 可以完整获取原始响应数据
- 对现有代码影响较小

**缺点**：
- 需要维护 HTTP 客户端代码
- 需要处理响应体的读取和重建

### 方案2: 修改 LangChain 的响应解析逻辑

直接修改 LangChain 的 `ChatOpenAI` 类，在解析响应时保留 `reasoning_content` 字段。

**优点**：
- 从根本上解决问题
- 思考过程会直接出现在响应对象中

**缺点**：
- 需要修改第三方库代码
- 升级 LangChain 时可能需要重新修改
- 不符合最佳实践

### 方案3: 使用回调处理器访问原始响应（当前实现）

在 `on_llm_start` 中存储 run_id，然后通过某种方式（如全局存储）关联原始响应。

**优点**：
- 不需要修改 HTTP 客户端
- 实现相对简单

**缺点**：
- 需要找到合适的方式来获取原始响应
- 可能需要使用 monkey patch 或装饰器

## 当前实现状态

当前代码已经添加了从全局存储中提取 `reasoning_content` 的逻辑，但还需要：

1. **实现 HTTP 响应拦截器**
   - 在 `infrastructure/llm/client.py` 中创建自定义 HTTP 客户端
   - 拦截 API 响应并提取 `reasoning_content`
   - 将 `reasoning_content` 存储到全局字典中，使用 run_id 作为 key

2. **在回调处理器中关联 run_id**
   - 在 `on_llm_start` 中生成或获取 run_id
   - 在 HTTP 拦截器中，需要能够将响应与 run_id 关联起来

## 推荐的实现步骤

### 步骤1: 创建响应拦截器

```python
# infrastructure/llm/response_interceptor.py
import httpx
import json
from typing import Dict, Any

_raw_responses: Dict[str, Dict[str, Any]] = {}

class ResponseInterceptor(httpx.BaseTransport):
    """拦截 HTTP 响应，提取 reasoning_content"""
    
    def __init__(self, transport: httpx.BaseTransport):
        self._transport = transport
    
    def handle_request(self, request: httpx.Request) -> httpx.Response:
        response = self._transport.handle_request(request)
        
        if "/chat/completions" in str(request.url):
            try:
                response_body = response.read()
                response_data = json.loads(response_body.decode('utf-8'))
                
                # 提取 reasoning_content
                if "choices" in response_data:
                    # 存储原始响应（需要关联 run_id）
                    # 这里需要找到合适的方式来关联 run_id
                    pass
                
                # 重建响应对象
                response = httpx.Response(
                    status_code=response.status_code,
                    headers=response.headers,
                    content=response_body,
                    request=request,
                )
            except Exception as e:
                logger.debug(f"拦截响应失败: {e}")
        
        return response
```

### 步骤2: 在 client.py 中使用自定义客户端

```python
# infrastructure/llm/client.py
from infrastructure.llm.response_interceptor import create_http_client_with_interceptor

def get_llm(...):
    # ...
    http_client = create_http_client_with_interceptor()
    params["http_client"] = http_client
    # ...
```

### 步骤3: 在回调处理器中提取

```python
# infrastructure/observability/llm_logger.py
from infrastructure.llm.response_interceptor import _raw_responses

def on_llm_end(self, response: LLMResult, **kwargs: Any):
    run_id = str(kwargs.get("run_id", ""))
    raw_response = _raw_responses.get(run_id)
    if raw_response:
        # 提取 reasoning_content
        reasoning_content = extract_reasoning_from_raw_response(raw_response)
```

## 临时解决方案

如果暂时无法实现完整的拦截器，可以：

1. **直接调用 API**：对于需要思考过程的场景，直接使用 `httpx` 调用 API，而不是通过 LangChain
2. **使用日志级别**：在开发/调试时，启用更详细的日志，直接查看原始 API 响应
3. **等待 LangChain 更新**：关注 LangChain 的更新，看是否会支持 `reasoning_content` 字段

## 参考文档

- [火山引擎 DeepSeek R1 模型文档](https://www.volcengine.com/docs/82379/1554373)
- [LangChain ChatOpenAI 文档](https://python.langchain.com/docs/integrations/chat/openai)
- 项目代码：`infrastructure/observability/llm_logger.py`
- 测试代码：`cursor_test/LLM_Thinking/test_reasoning_extraction.py`

